<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="25">
            <Title>_Recovered Files</Title>
        </Document>
        <Document ID="19">
            <Title>Conclusion</Title>
            <Text>In this project. a reimplementation of the NMMSO algorithm in R was shown. As part of this project also, a reimplementation of the CEC Benchmarking Suite was created. It was shown that it is possible to translate a Matlab program into R while keeping the comparability at a high level. In the programming project approaches have been shown and tested to overcome common differences in the two programming language to implement an existing algorithm in both languages. 

Even though this implementation is not the most performant and certainly had its problems while being implemented a recreation of the NMMSO algorithm and the CEC benchmarking tool have been done which work reliably and provide similar results to the original implementations. 

A foundation to further use this both packages has been set, which enables further testing and invite to reuse the nmmso.R for different uses. Especially an improvement in the implementations performance could be interesting. In this matter it could already help to minimize the I/O actions, which are currently done to gather the benchmarking data. But also other improvements to better match the style of R could help.</Text>
        </Document>
        <Document ID="21">
            <Title>cec2013-niching-benchmark-tech-report</Title>
            <Text>Benchmark Functions for CEC’2013 Special Session and Competition on Niching Methods for Multimodal Function Optimization Xiaodong Li, Andries Engelbrecht, Michael G. Epitropakis Evolutionary Algorithms (EAs) in their original forms are usually designed for locating a single global solution. These algorithms typically converge to a single solution because of the global selection scheme used. Nevertheless, many real- world problems are “multimodal” by nature, i.e., multiple satisfactory solutions exist. It may be desirable to locate many such satisfactory solutions so that a decision maker can choose one that is most proper in his/her problem domain. Numerous techniques have been developed in the past for locating multiple optima (global or local). These techniques are commonly referred to as “niching” methods. A niching method can be incorporated into a standard EA to promote and maintain formation of multiple stable subpopulations within a single population, with an aim to locate multiple globally optimal or suboptimal solutions. Many niching methods have been developed in the past, including crowding [1], fitness sharing [2], deterministic crowding [3], derating [4], restricted tournament selection [5], parallelization [6], stretching and deflation [7], clustering [8], clearing [9], and speciation [10], etc. Although these niching methods have been around for many years, further advances in this area have been hindered by several obstacles: most studies focus on very low dimen- sional multimodal problems (2 or 3 dimensions), therefore it is difficult to assess these methods’ scalability to high dimensions; some niching methods introduces new parameters which are difficult to set, making these methods difficult to use; different benchmark test functions or different variants of the same functions are used, hence comparing the performance of different niching methods is difficult. We believe it is now time to adopt a unifying framework for evaluating niching methods, so that further advance in this area can be made with ease. In this technical report, we put together 20 benchmark test functions (including several identical functions with different dimension sizes), with dif- ferent characteristics, for evaluating niching algorithms. The first 10 benchmark functions are simple, well known and widely used functions, largely based on some recent studies on niching [11], [12], [13]. The remaining benchmark functions Xiaodong Li is with the School of Computer Science and IT, RMIT University, VIC 3001, Melbourne, Australia, email: xiaodong.li@rmit.edu.au Andries Engelbrecht is with the Department of Computer Science, School of Information Technology, University of Pretoria, Pretoria 0002, South Africa, email: engel@cs.up.ac.za Michael G. Epitropakis is with the Department of Computing Science and Mathematics, School of Natural Sciences, University of Stirling, Stirling FK9 4LA, Scotland, email: mge@cs.stir.ac.uk are more complex and follow the paradigm of composition functions defined in the IEEE CEC 2005 special session on real-parameter optimization [14]. Several of the test functions included here are scalable to dimension, and the number of global optima can be adjusted freely by the user. Performance measures are also defined and suggested here for comparing different niching methods. The source codes of the benchmark test functions are made avail- able in Matlab, Java and C++ source codes. The competition files can be downloaded from the CEC’2013 special session on niching methods website1 . In the following sections, we will describe the mathematical formula and properties of the included multimodal benchmark test functions, evaluation criteria, and the ranking method for entries submitted to this competition. I. SUMMARY OF TEST FUNCTIONS This benchmark set includes the following 20 multimodal test functions: • F1 : • F2 : • F3 : • F4 : • F5 : • F6 : • F7: Vincent (2D, 3D) • F8: Modified Rastrigin - All Global Optima (2D) • F9 : Composition Function 1 (2D) Five-Uneven-Peak Trap (1D) Equal Maxima (1D) Uneven Decreasing Maxima (1D) Himmelblau (2D) Six-Hump Camel Back (2D) Shubert (2D, 3D) • F10 : Composition Function 2 (2D) • F11: Composition Function 3 (2D, 3D, 5D, 10D) • F12: Composition Function 4 (3D, 5D, 10D, 20D) These multimodal test functions have following properties: • All test functions are formulated as maximization prob- lems; • F1, F2 and F3 are simple 1D multimodal functions; • F4 and F5 are simple 2D multimodal functions. These functions are not scalable; • F6 to F8 are scalable multimodal functions. The number of global optima for F6 and F7 are determined by the dimension D. However, for F8, the number of global op- tima is independent from D, therefore can be controlled by the user. • F9 to F12 are scalable multimodal functions constructed by several basic functions with different properties. F9 1URL: http://goanna.cs.rmit.edu.au/∼xiaodong/cec13-niching/ 1 2 ￼￼￼￼￼￼￼￼￼￼￼￼￼and F10 are separable, and non-symmetric, while F11 and F12 are non-separable, non-symmetric complex mul- timodal functions. The number of global optima in all composition functions is independent from D, therefore can be controlled by the user. II. DEFINITIONS OF THE TEST FUNCTIONS 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Equal Maxima. ￼￼￼￼￼￼￼￼￼￼￼￼A. F1: Five-Uneven-Peak Trap 80(2.5 − x)   64(x − 2.5)   64(7.5 − x) 28(x − 7.5) 28(17.5 − x)   32(x − 17.5)   32(27.5 − x) 80(x − 27.5) for0≤x&lt;2.5, for2.5≤x&lt;5.0, for5.0≤x&lt;7.5, for7.5≤x&lt;12.5, for12.5≤x&lt;17.5, for17.5≤x&lt;22.5, for22.5≤x&lt;27.5, for27.5≤x≤30. Fig. 2. ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼F1(x) = Properties: Properties: – Variable ranges: x ∈ [0, 1]; – No. of global optima: 1; – No. of local optima: 4; This function was originally proposed in [15]. See Fig. 3. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fig. 3. Uneven Decreasing Maxima. D. F4 : Himmelblau F4(x,y)=200−(x2 +y−11)2 −(x+y2 −7)2. Properties: – Variableranges:x,y∈[−6,6]; – No. of global optima: 4; – No. of local optima: 0; This is an inverted version of Himmelblau function [15]. It has 4 global optima with 2 closer to each other than the other 2. There are no local optima, as shown in Fig. 4. E. F5: Six-Hump Camel Back F5(x,y)=−4[(4−2.1x2 + 3 )x2 +xy+(4y2 −4)y2]. – Variable ranges: x ∈ [0, 30]; – No. of global optima: 2; – No. of local optima:3. ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼This function was originally proposed in [10]. See Fig. 1. 200 180 160 140 120 100 80 60 40 20 0 0 5 10 15 20 25 30 Five-Uneven-Peak Trap. ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼Fig. 1. B. F2: Equal Maxima F2(x) = sin6(5πx). Properties: – Variable ranges: x ∈ [0, 1]; – No. of global optima: 5; – No. of local optima: 0; This function was originally proposed in [15]. See Fig. 2. C. F3: Uneven Decreasing Maxima x−0.08 2 0.854 x4 F (x) = exp −2log(2) 3 sin6(5π(x3/4−0.05)). ￼￼ ￼￼where there are 18 global optima in 9 pairs (each with an objective function value of 186.73), with each pair very close to each other, but the distance between any pair is much greater. There are in total 760 global and local optima. 3 ￼￼￼￼￼￼￼￼￼￼￼￼Fig. 4. Himmelblau. ￼Properties: – Variable ranges: x ∈ [−1.9, 1.9]; y ∈ [−1.1, 1.1]; – No. of global optima: 2; – No. of local optima: 2; This function was originally proposed in [16]. The function has 2 global optima as well as 2 local optima. See Fig. 5. Fig. 6. Shubert 2D function. sin(10log(xi)) ￼G. F7: Vincent F7(⃗x) = 1 D ￼￼D i=1 Properties: ￼– Variable range: xi ∈ [0.25,10]D,i = 1,2,...,D – No. of global optima: 6D; – No. of local optima: 0; This is an inverted version of the Vincent function [17], which has 6D global optima (each with an objective function value of 1.0), but unlike the regular distances between global optima in F6, in Vincent function the global optima have vastly different spacing between them. In addition, the Vincent function has no local optima. Fig. 7 shows an example of the Vincent 2D function. ￼￼￼￼￼￼Fig. 5. Six-Hump Camel Back. ￼F. F6: Shubert F6(⃗x) = − Di=1 5j=1 jcos[(j + 1)xi + j]. Properties: – Variable ranges: xi ∈ [−10,10]D,i = 1,2,...,D; – No.ofglobaloptima:D·3D; – No. of local optima: many; This function is an inverted version of the Shubert function [16], [10], where there are n3D global optima unevenly distributed. These global optima are divided into 3D groups, with each group having D global optima being close to each other. Fig. 6 shows an example of the Shubert 2D function, Fig. 7. Vincent 2D function. ￼￼￼ H. F : Modified Rastrigin - All Global Optima weight, o⃗ is the new shifted optimum of each fˆ , M is the 8iii D (10 + 9cos(2πkixi)). i=1 – Variable ranges: xi ∈ [0,1]D,i = 1,2,...,D; – No. of global optima: Di=1 ki; – No. of local optima: 0; a parameter which is used to stretch (λi &gt; 1) or compress (λ &lt; 1) each fˆ function. The composition function includes F9(⃗x) = − Properties: ii two bias parameters biasi and fj . The former defines a This is a modified Rastrigin function according to [13]. The total number of global optima is M = D k . In [13], a optimum is the global optimum, while the latter defines a function value bias for the constructed composition function. Here, we set the biasi = 0,∀i ∈ {1,2,...,n}, thus the global optimum of each basic function is a global optimum of the composition function. In addition, we set fj = 0, as such bias in each composition function, all global optima have fitness values equal to zero. The weight wi of each basic function can be easily calcu- lated based on the following equations: i=1 i problem instance D = 16 was used, with the following setting: ki = 1, for i = 1−3,5−7,9−11,13−15, and k4 = 2,k8 = 2,k12 = 3,k16 = 4. In this case, there are 48 evenly distributed global optima, each having an identical objective value of f = 16. Fig. 8 provides a problem instance of this function in 2D (where D = 2,k1 = 3,k2 = 4), in which case the total number of optima is 12. Dk=1(xk − oik)2 wi=exp − 2Dσ2 , linear transformation (rotation) matrix of each fˆ, and λ is ii bias function value bias for each basic function and denotes which 4 ￼wi = wi wi(1 − max(wi)10) otherwise. i wi = max(wi) ￼￼Finally, the weights are normalized according to wi = wi/ ni=1 wi. The parameter σi controls the coverage range of each basic function, with small values to produce a narrow coverage range to the corresponding fˆ . The pool of basic functions may include functions with different properties, characteristics and heights. As such to have a better mixture of the basic functions a normalization procedure is incorporated. The normalized function fˆ , can be i defined as: fˆ(·) = Cf (·)/|fi |, where C is a predefined i i max constant (C = 2000) and fmiax is estimated using: fmiax = fi ((x⋆/λi)Mi), with x⋆ = [5,5,...,5]. The pool of basic functions that we have used to construct the composition functions includes the following: ￼i ￼￼￼￼Fig. 8. Modified Rastrigin - All Global Optima 2D function. D f S ( ⃗x ) = x 2i . i=1 I. Composition functions In this section, we first describe the general framework for constructing multimodal composition functions with several global optima, then present the new composition functions used in this technical report. More specifically, a D-dimensional, composition function CFj : AD ⊂ RD → R can be generally constructed as a weighted aggregation of n basic functions fi : AD ⊂ RD → R. Each basic function is shifted to a new position inside the optimization space AD and can be either rotated through a linear transformation matrix or used as is. Thus, a composition function CFj is calculated according the following equation: n CF(⃗x)= w fˆ((⃗x−o⃗)/λ·M)+bias +fj , Grienwank’s function: – – – – Sphere function: fG(⃗x) = i=1 Rastrigin’s function: 4000 − cos √i i=1 + 1. D x2i D xi ￼￼￼j iiiiiibias i=1 k k where n is the number of basic functions used to construct −D α cos 2πβ (0.5) , i-th basic function, i ∈ {1, 2, . . . , n}, wi is the corresponding where α = 0.5, β = 3, and kmax = 20. the composition function, fˆ denotes a normalization of the i k=0 D fR(⃗x)= x2i −10cos(2πxi)+10 . i=1 Weierstrass function: D kmax fW(⃗x)= αk cos 2πβk(xi +0.5) i=1 k=0 kmax 5 ￼– Expanded Griewank’s plus Rosenbrock’s function (EF8F2): ￼ D x2i D xi F8(⃗x)= 4000 − cos √i i=1 i=1 D−1 +1, ￼￼￼￼F2(⃗x)= 100(x2i −xi+1)2 +(xi −1)2 , i=1 EF8F2(⃗x)=F8F2(x1,x2,...,xD) =F8(F2(x1,x2))+F8(F2(x2,x3))+... +F8(F2(xD−1,xD))+F8(F2(xD,x1)) It is clear that the aforementioned basic functions do not incorporate either shifted positions, or linear transfor- mations (rotations). Thus, in order to calculate for example fS ((⃗x − o⃗i)/λi · Mi) , one can easily first calculate ⃗z = (⃗x − o⃗i)/λi · Mi and subsequently fS(⃗z). It has to be noted that all composition functions are formulated as maximization problems. Note on the implementation: The implementation of the composition functions has been mostly based on the source code of the IEEE CEC 2005 competition on real-parameter optimization [14]. However, the source code has been re- implemented from scratch in a simplified and scalable way. It has to be noted that, similar implementation of composition functions with multiple global optima was proposed in [18]. However, our implementation of the composition functions are different in many ways: new structures, parameters, rotation matrices, and shifted optima positions. J. F9 : Composition Function 1 Composition Function 1 (CF1) is constructed based on six basic functions (n = 6), thus it has six global optima in the optimization box AD = [−5, 5]D . The basic functions used here include the following: – f1 − f2 : Grienwank’s function, – f3 − f4 : Weierstrass function, and – f5 − f6 : Sphere function. The composition function is constructed based on the follow- ing parameter settings: – σi = 1,∀i ∈ {1,2,...,n}, – ⃗λ = [1,1,8,8,1/5,1/5], – Mi are identity matrices ∀i ∈ {1,2,...,n}. Fig. 9 shows the 2D version of CF1. Properties: – Multi-modal, – Shifted, – Non-Rotated, – Non-symmetric, – Separable near the global optima, – Scalable, Fig. 9. Composition Function 1. ￼￼￼￼K. F10 : Composition Function 2 Composition Function 2 (CF2) is constructed based on eight basic functions (n = 8), thus it has eight global optima in the optimization here include – f1 − f2 – f3 − f4 – f5 − f6 – f7 − f8 box AD = [−5, 5]D . The basic functions used the following: : Rastrigin’s function, : Weierstrass function, : Griewank’s function, and : Sphere function. The composition function is constructed based on the follow- ing parameter settings: – σi = 1,∀i ∈ {1,2,...,n}, – ⃗λ=[1,1,10,10,1/10,1/10,1/7,1/7], – Mi are identity matrices ∀i ∈ {1,2,...,n}. Fig. 10 shows the 2D version of CF2. ￼￼￼￼￼￼￼– Numerous local optima, – Different function’s properties are mixed together, – Sphere Functions give two flat areas for the function, – In the optimization box AD = [−5, 5]D , there are Fig. 10. Composition Function 2. six global optima x⃗⋆i = o⃗i,i ∈ {1,2,...,n} with CF1(x⃗⋆i ) = Properties: – Multi-modal, – Shifted, 0,∀i ∈ {1,2,...,n}. – Non-Rotated, – Non-symmetric, – Separable near the global optima, – Scalable, – Numerous local optima, – Different function’s properties are mixed together, – In the optimization box AD = [−5,5]D, there are eight global optima x⃗⋆i = o⃗i,i ∈ {1,2,...,n} with CF2(x⃗⋆i ) = 0,∀i ∈ {1,2,...,n}. L. F11 : Composition Function 3 Composition Function 3 (CF3) is constructed based on six basic functions (n = 6), thus it has six global optima in the optimization box AD = [−5, 5]D . The basic functions used here include the following: – f1 − f2 : EF8F2 function, – f3 − f4 : Weierstrass function, and – f5 − f6 : Griewank’s function. The composition function is constructed based on the follow- ing parameter settings: – ⃗σ = [1,1,2,2,2,2], – ⃗λ = [1/4,1/10,2,1,2,5], – Mi are different linear transformation (rotation) matrices with condition number one. Fig. 11 shows the 2D version of the CF3. M. F12 : Composition Function 4 Composition Function 4 (CF4) is constructed based on eight basic functions (n = 8), thus it has eight global optima in the optimization box AD = [−5, 5]D . The basic functions used here include the following: – f1 − f2 : Rastrigin’s function, – f3 − f4 : EF8F2 function, – f5 − f6 : Weierstrass function, and – f7 − f8 : Griewank’s function. The composition function is constructed based on the follow- ing parameter settings: – ⃗σ = [1,1,1,1,1,2,2,2], – ⃗λ = [4, 1, 4, 1, 1/10, 1/5, 1/10, 1/40], – Mi are different linear transformation (rotation) matrices with condition number one. Fig. 12 shows the 2D version of the CF4. 6 ￼￼￼￼￼￼￼￼￼￼￼￼￼Fig. 11. Composition Function 3. Properties: – Multi-modal, – Shifted, – Rotated, – Non-symmetric, – Non-separable, – Scalable, – A huge number of local optima, – Different function’s properties are mixed together, – In the optimization box AD = [−5,5]D, there are eight global optima x⃗⋆i = o⃗i,i ∈ {1,2,...,n} with CF4(x⃗⋆i ) = 0,∀i ∈ {1,2,...,n}. III. PERFORMANCE MEASURES A. How to determine if all global optima are found? Our objective is to compare different niching algorithms’ capability to locate all global optima. To achieve this, first we need to specify a level of accuracy (typically 0 &lt; ε ≤ 1), a threshold value under which we would consider a global optimum is found. Second, we assume that for each test function, the following information is available: Properties: – Multi-modal, – Shifted, – Rotated, – Non-symmetric, – Non-separable, – Scalable, – A huge number of local optima, – Different function’s properties are mixed together, – In the optimization box AD = [−5, 5]D , there are six global optima x⃗⋆i = o⃗i,i ∈ {1,2,...,n} with CF3(x⃗⋆i ) = 0,∀i ∈ {1,2,...,n}. Fig. 12. Composition Function 4. ￼ • The number of global optima; • The fitness of the global optima (or peak height), which is known or can be estimated; • A niche radius value that can sufficiently distinguish two closest global optima. This information is required by Algorithm 1 (based on a previously proposed procedure [19]), to determine if a niching algorithm has located all the global optima (Table IV provides an example of how this information is used for performance measures in this competition). Basically, at the end of an optimization run, Algorithm 1 is invoked to check all the individuals on the sorted list Lsorted, starting from the best- fit individual. In the first iteration, as long as this best-fit individual is within ε distance from the fitness of the global optima ph, it will be added to the solution list S (which is initially empty). In the next iteration, the next best-fit individual from Lsorted is first assigned to p. We then check if the fitness of p is close enough to that of the global optima (i.e., if d(ph, f it(p)) ≤ ε). If p is close enough (in other words, p is a potential solution), then next we check if p is within the niche radius r from all the current solutions in the solution list S. If it is not, then p is considered as a new solution (i.e., a new global optimum is found), and will be added to S. Otherwise p is not considered as a distinct solution, and will be skipped. This process is repeated until all the individuals on Lsorted have been checked. It is important to note that Algorithm 1 is only used for performance measurement in determining if a sufficient number of global optima has been found, but not in any part of the optimization procedure. The output of Algorithm 1 is S, a solution list containing all the distinct global optima found. As long as r is set to a value not greater than the distance between 2 closest global optima, individuals on two found global optima would be treated as two different solutions. Since the exact number of global optima is known a priori, we can measure a niching algorithm’s performance in terms of the peak ratio, success rate, and averaged number of evaluations required to achieve a given accuracy ε for locating all global optima over multiple runs. These will be described in the following sections. B. Peak ratio and success rate We use peak ratio (PR) [20] and success rate (SR) as two performance measures, to evaluate the performance of a niching algorithm over multiple runs. Given a fixed maximum number of function evaluations (MaxFEs) and a required accuracy level ε, PR measures the average percentage of all known global optima found over multiple runs: Algorithm 1: The algorithm for determining if all global optima are found. where NSR denotes the number of successful runs. C. Convergence speed We measure the convergence speed of a niching algorithm by counting the number of function evaluations (FEs) required to locate all known global optima, at a specified accuracy level ε. We calculate the average FEs over multiple runs: AveFEs = NR FEi run=1 , (3) NR 7 ￼￼￼input : Lsorted - a list of individuals (candidate solu- tions) sorted in decreasing fitness values; ε - accuracy level; r - niche radius; ph - the fitness of global optima (or peak height) output: S - a list of best-fit individuals identified as solutions begin S = ∅; while not reaching the end of Lsorted do Get best unprocessed p ∈ Lsorted; found ← FALSE; if d(ph, f it(p)) ≤ ε) then for all s ∈ S do if d(s, p) ≤ r then found ← TRUE; break; end end if not found then let S ← S ∪ {p}; end end end end ￼￼￼￼￼￼￼￼PR = NR NPFi run=1 , (1) where F Ei denotes the number of evaluations used in the i-th run. If the algorithm cannot locate all the global optima by the MaxFEs, then MaxFEs is used when calculating the average FEs. IV. EXPERIMENTAL SETTINGS We evaluate niching methods by using a fixed amount of MaxFEs. A user is allowed to use any population size, but only a fixed amount of MaxFEs is allowed to be used as a given computational budget. A. Peak ratio and success rate PR and SR are calculated according to equations (1) and (2). The following parameter settings are used: • Level of accuracy (ε): {1.0E-01,1.0E-02, . . . , 1.0E-05}; • Niche radius (r): See Table IV; ￼NKP ∗NR where N P Fi denotes the number of global optima found in the end of the i-th run, NKP the number of known global optima, and NR the number of runs. SR measures the percentage of successful runs (a successful run is defined as a run where all known global optima are found) out of all runs: SR = NSR, (2) NR ￼ • Initialization: Uniform random initialization within the search space; • Termination of a run: Terminate when reaching MaxFEs. • Number of runs: 50. Note that ε and r are to be used only at the end of a run, for evaluations of the final solutions. Table I shows different MaxFEs used for the 3 ranges of test functions. TABLE I MAXFES USED FOR 3 RANGES OF TEST FUNCTIONS. TABLE IV PARAMETERS USED FOR PERFORMANCE MEASUREMENT. 8 ￼￼￼r ￼Peak height ￼￼0.01 ￼200.0 ￼￼0.01 ￼1.0 ￼￼0.01 ￼1.0 ￼￼0.01 ￼200.0 ￼￼0.5 ￼1.03163 ￼￼0.5 ￼186.731 ￼￼0.2 ￼1.0 ￼￼0.5 ￼2709.0935 ￼￼0.2 ￼1.0 ￼￼0.01 ￼-2.0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼0.01 ￼0 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼Range of functions F1 to F5 (1D or 2D) F6 to F11 (2D) MaxFEs 5.0E+04 2.0E+05 Function F1 (1D) F2 (1D) F3 (1D) F4 (2D) F5 (2D) F6 (2D) F7 (2D) F6 (3D) F7 (3D) F8 (2D) F9 (2D) F10 (2D) F11 (2D) F11 (3D) F12 (3D) F11 (5D) F12 (5D) F11 (10D) F12 (10D) F12 (20D) No. global optima 2 5 1 4 2 18 36 81 216 12 6 8 6 6 8 6 8 6 8 8 ￼￼￼￼￼￼￼￼F6 to F12 (3D or higher) ￼ 4.0E+05 ForF8 (2D),wesetk1 =3andk2 =4,asshownin Fig. 8. Table II shows an example of presenting PR and SR values of a typical niching algorithm. Note that in this example the PR and SR values are calculated based on the results of a baseline model, DE/nrand/1/bin algorithm (see details in a section below). B. Convergence speed Convergence speed is calculated according to equation (3), using the same MaxFEs settings in Table I. The accuracy level ε is set to 1.0E-04. Other parameters are the same as in Table IV. Table V presents the convergence speed results of the DE/nrand/1/bin algorithm. To further illustrate the behaviour of the niching algorithm, we can record the number of global optima found at different iteration steps of a run. We recommend to use figures to show the mean global optima found averaged over 50 runs, on 5 or 6 different test functions of your choice. We encourage authors to follow a recent paper on niching [21] on how to better present results. C. Baseline models To facilitate easy comparisons for participants in the competition, we use as baseline models two Differen- tial Evolution (DE) niching variants, the recently proposed DE/nrand/1/bin algorithm [21] and the well known Crowding DE/rand/1/bin [20]. DE/nrand/1/bin is a simple DE algorithm which incorporates spatial information about the neighborhood of each potential solution to produce a niching formation. On the other hand, Crowding DE/rand/1/bin produces niching formation by incorporating the crowding technique to maintain a better population diversity and therefore to prevent premature convergence to an optimum. The results of the two baseline models are presented in Tables II, III, and Tables V, VI. Please note that we did not conduct any fine-tuning on the parameters of the baseline algorithms. Instead, we used the following default parameters: population size NP = 100, F = 0.5, CR = 0.9, and the crowding factor equals to the population size CF = NP. TABLE V CONVERGENCE SPEEDS OF THE DE/NRAND/1/BIN ALGORITHM (WITH ACCURACY LEVEL ε = 1.0E-04). ￼￼￼￼￼￼￼￼￼￼￼￼￼F1 (1D) F2 (1D) ￼￼F3 (1D) ￼F4 (2D) ￼￼22886.0 2689.056 ￼￼1552.0 386.106 ￼￼1258.0 781.179 ￼￼￼13610.0 1399.453 ￼￼￼F6 (2D) F7 (2D) ￼￼F6 (3D) ￼F7 (3D) ￼￼200000.0 0.000 200000.0 0.000 ￼￼400000.0 0.000 ￼400000.0 0.000 ￼￼F9 (2D) F10 (2D) ￼￼F11 (2D) ￼F11 (3D) ￼￼200000.0 0.000 ￼￼181658.0 42543.630 ￼￼200000.0 0.000 ￼￼￼400000.0 0.000 ￼F11 (5D) F12 (5D) F11 (10D) F12 (10D) ￼￼400000.0 0.000 ￼￼400000.0 0.000 ￼￼400000.0 0.000 ￼￼￼400000.0 0.000 ￼￼Function Mean St. D. Function F5 (2D) 3806.0 618.890 F8 (2D) 9858.0 ￼￼￼Mean St. D. ￼ ￼ ￼ ￼ ￼ 833.015 ￼Function Mean St. D. Function ￼ ￼ ￼ ￼ ￼ F12 (20D) F12 (3D) 400000.0 0.000 ￼￼￼Mean St. D. 400000.0 0.000 ￼￼TABLE VI CONVERGENCE SPEEDS OF THE CROWDING DE/RAND/1/BIN ALGORITHM (WITH ACCURACY LEVEL ε = 1.0E-04). ￼￼￼F1 (1D) ￼F2 (1D) ￼F3 (1D) ￼F4 (2D) ￼￼50000.0 0.000 ￼￼￼3386.0 1368.749 ￼￼2576.0 2625.974 ￼￼41666.0 3772.598 ￼￼￼F6 (2D) ￼F7 (2D) ￼F6 (3D) ￼F7 (3D) ￼￼200000.0 0.000 ￼￼￼200000.0 0.000 ￼￼400000.0 0.000 ￼￼400000.0 0.000 ￼￼￼F9 (2D) ￼F10 (2D) ￼F11 (2D) ￼F11 (3D) ￼￼200000.0 0.000 ￼￼￼200000.0 0.000 ￼￼200000.0 0.000 ￼￼400000.0 0.000 ￼F11 (5D) F12 (5D) F11 (10D) F12 (10D) ￼￼400000.0 0.000 ￼￼￼400000.0 0.000 ￼￼400000.0 0.000 ￼￼400000.0 0.000 ￼￼Function Mean St. D. Function Mean St. D. Function Mean F5 (2D) 12980.0 2046.799 F8 (2D) 30306.0 1984.677 F12 (3D) 400000.0 0.000 ￼￼￼￼￼St. D. Function ￼ ￼ ￼ ￼ ￼ F12(20D) ￼￼Mean St. D. 400000.0 0.000 ￼￼V. RANKING METHOD We will use peak ratio values in Table II as our key criterion to rank algorithms submitted to this competition. The top algorithm is the one that obtains the best average peak ratio, across all test functions and 5 accuracy levels. If there is a tie, then the algorithm having the lower AveFEs in Table V will be the winner. REFERENCES [1] K.A.DeJong,“Ananalysisofthebehaviorofaclassofgeneticadaptive systems.” Ph.D. dissertation, University of Michigan, 1975. [2] D. E. Goldberg and J. Richardson, “Genetic algorithms with sharing for multimodal function optimization,” in Proc. of the Second International Conference on Genetic Algorithms, J. Grefenstette, Ed., 1987, pp. 41–49. [3] S. W. Mahfoud, “Crowding and preselection revisited,” in Parallel problem solving from nature 2, R. Ma ̈nner and B. Manderick, Eds. Amsterdam: North-Holland, 1992, pp. 27–36. [Online]. Available: citeseer.ist.psu.edu/mahfoud92crowding.html [4] D. Beasley, D. R. Bull, and R. R. Martin, “A sequential niche technique for multimodal function optimization,” Evolutionary Computation, vol. 1, no. 2, pp. 101–125, 1993. [Online]. Available: citeseer.ist.psu. edu/beasley93sequential.html [5] G. R. Harik, “Finding multimodal solutions using restricted tournament selection,” in Proc. of the Sixth International Conference on Genetic Algorithms, L. Eshelman, Ed. San Francisco, CA: Morgan Kaufmann, 1995, pp. 24–31. [Online]. Available: citeseer.ist.psu.edu/harik95finding. html [6] M. Bessaou, A. Pe ́trowski, and P. Siarry, “Island model cooperating with speciation for multimodal optimization,” in Parallel Problem Solving from Nature - PPSN VI 6th International Conference, H.-P. S. et al., Ed. Paris, France: Springer Verlag, 16-20 2000. [Online]. Available: citeseer.ist.psu.edu/bessaou00island.html [7] K.E.ParsopoulosandM.N.Vrahatis,“Onthecomputationofallglobal minimizers through particle swarm optimization,” IEEE Transactions on Evolutionary Computation, vol. 8, no. 3, pp. 211–224, 2004. [8] X. Yin and N. Germay, “A fast genetic algorithm with sharing scheme using cluster analysis methods in multi-modal function optimization,” in the International Conference on Artificial Neural Networks and Genetic Algorithms, 1993, pp. 450–457. [9] A. Pe ́trowski, “A clearing procedure as a niching method for genetic algorithms,” in Proc. of the 3rd IEEE International Conference on Evolutionary Computation, 1996, pp. 798–803. [10] J.-P. Li, M. E. Balazs, G. T. Parks, and P. J. Clarkson, “A species conserving genetic algorithm for multimodal function optimization,” Evol. Comput., vol. 10, no. 3, pp. 207–234, 2002. [11] X.Li,“Nichingwithoutnichingparameters:Particleswarmoptimization using a ring topology,” IEEE Trans. on Evol. Comput., vol. 14, no. 1, pp. 150 – 169, February 2010. [12] K. Deb and A. Saha, “Finding multiple solutions for multimodal optimization problems using a multi-objective evolutionary approach,” in Proceedings of the 12th annual conference on Genetic and evolutionary computation, ser. GECCO ’10. New York, NY, USA: ACM, 2010, pp. 447–454. [13] A. Saha and K. Deb, “A bi-criterion approach to multimodal optimiza- tion: self-adaptive approach,” in Proceedings of the 8th international conference on Simulated evolution and learning, ser. SEAL’10. Berlin, Heidelberg: Springer-Verlag, 2010, pp. 95–104. [14] P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y. P. Chen, A. Auger, and S. Tiwari, “Problem definitions and evaluation criteria for the CEC 2005 special session on real-parameter optimization,” Nanyang Technological University and KanGAL Report #2005005, IIT Kanpur, India., Tech. Rep., 2005. [15] K. Deb, “Genetic algorithms in multimodal function optimization (mas- ter thesis and tcga report no. 89002),” Ph.D. dissertation, Tuscaloosa: University of Alabama, The Clearinghouse for Genetic Algorithms, 1989. [16] Z. Michalewicz, Genetic Algorithms + Data Structures = Evolution Programs. New York: Springer-Verlag, New York, 1996. [17] O. Shir and T. Ba ̈ck, “Niche radius adaptation in the cms-es niching algorithm,” in Parallel Problem Solving from Nature - PPSN IX, 9th International Conference (LNCS 4193). Reykjavik, Iceland: Springer, 2006, pp. 142 – 151. [18] B.-Y. Qu and P. N. Suganthan, “Novel multimodal problems and differential evolution with ensemble of restricted tournament selection,” in Proceedings of the IEEE Congress on Evolutionary Computation, CEC 2010. Barcelona, Spain: IEEE, 18-23 July 2010, pp. 1–7. [19] D.ParrottandX.Li,“Locatingandtrackingmultipledynamicoptimaby a particle swarm model using speciation,” IEEE Trans. on Evol. Comput., vol. 10, no. 4, pp. 440–458, August 2006. [20] R. Thomsen, “Multimodal optimization using crowding-based differen- tial evolution,” in Proceedings of the 2004 IEEE Congress on Evolu- tionary Computation. Portland, Oregon: IEEE Press, 20-23 Jun. 2004, pp. 1382–1389. [21] M. Epitropakis, V. Plagianakos, and M. Vrahatis, “Finding multiple global optima exploiting differential evolution’s niching capability,” in IEEE Symposium on Differential Evolution, 2011. SDE 2011. (IEEE Symposium Series on Computational Intelligence), Paris, France, April 2011, p. 80–87. 9 TABLE II PEAK RATIOS AND SUCCESS RATES OF THE DE/NRAND/1/BIN ALGORITHM. Accuracy level ε F5 (2D) SR 10 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼PR 1.000 1.000 1.000 1.000 1.000 ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼Accuracy level ε F8 (2D) SR F6 (2D) F7 (2D) F6 (3D) F7 (3D) ￼￼￼￼￼￼￼￼￼￼￼PR 1.000 1.000 0.998 1.000 1.000 PR SR PR SR PR SR PR SR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.980 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 0.450 0.438 0.440 0.434 0.000 0.000 0.000 0.000 0.000 0.000 0.347 0.346 0.349 0.337 0.333 0.000 0.000 0.000 0.000 0.000 0.108 0.105 0.113 0.112 0.113 0.000 0.000 0.000 0.000 0.000 0.097 0.095 0.099 0.095 0.094 0.000 0.000 0.000 0.000 0.000 ￼￼￼￼￼￼Accuracy level ε F12 (3D) SR F9 (2D) F10 (2D) F11 (2D) F11 (3D) ￼￼￼￼￼￼￼￼￼￼￼PR 0.522 0.535 0.507 0.502 0.507 PR SR PR SR PR SR PR SR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 0.683 0.673 0.683 0.673 0.670 0.000 0.000 0.000 0.000 0.000 0.855 0.837 0.815 0.815 0.777 0.240 0.220 0.140 0.160 0.100 0.667 0.667 0.667 0.667 0.667 0.000 0.000 0.000 0.000 0.000 0.667 0.667 0.667 0.667 0.667 0.000 0.000 0.000 0.000 0.000 ￼￼￼￼￼￼Accuracy level ε F12 (20D) SR F11 (5D) F12 (5D) F11 (10D) F12 (10D) ￼￼￼￼￼￼￼￼￼￼￼PR SR PR SR PR SR PR SR PR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 TABLE III PEAK RATIOS AND SUCCESS RATES OF THE CROWDING DE/RAND/1/BIN ALGORITHM. 0.677 0.663 0.663 0.663 0.657 0.000 0.000 0.000 0.000 0.000 0.345 0.325 0.295 0.290 0.287 0.000 0.000 0.000 0.000 0.000 0.403 0.343 0.323 0.270 0.250 0.000 0.000 0.000 0.000 0.000 0.227 0.167 0.152 0.125 0.127 0.000 0.000 0.000 0.000 0.000 0.130 0.127 0.130 0.125 0.123 ￼￼￼￼￼￼￼￼￼Accuracy level ε F5 (2D) SR F1 (1D) F2 (1D) F3 (1D) F4 (2D) ￼￼￼￼￼￼￼￼￼￼￼PR 1.000 1.000 1.000 1.000 1.000 PR SR PR SR PR SR PR SR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.000 0.710 0.090 0.020 0.000 1.000 0.500 0.000 0.000 0.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.995 0.420 1.000 1.000 1.000 0.980 0.040 ￼￼￼￼￼￼Accuracy level ε F8 (2D) SR F6 (2D) F7 (2D) F6 (3D) F7 (3D) ￼￼￼￼￼￼￼￼￼￼￼PR 1.000 1.000 1.000 1.000 1.000 PR SR PR SR PR SR PR SR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 1.000 1.000 0.999 0.972 0.107 0.000 1.000 0.980 0.740 0.000 0.000 0.703 0.724 0.715 0.709 0.716 0.000 0.000 0.000 0.000 0.000 0.847 0.835 0.716 0.290 0.038 0.000 0.000 0.000 0.000 0.000 0.271 0.272 0.274 0.274 0.270 0.000 0.000 0.000 0.000 0.000 ￼￼￼￼￼￼Accuracy level ε F12 (3D) SR PR 1.000 1.000 1.000 1.000 1.000 F1 (1D) F9 (2D) SR PR F2 (1D) F10 (2D) SR PR F3 (1D) F11 (2D) SR PR F4 (2D) F11 (3D) SR ￼￼￼￼￼￼￼￼￼￼￼PR 0.730 0.690 0.627 0.490 0.375 PR SR PR SR PR SR PR SR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 0.937 0.690 0.667 0.667 0.667 0.720 0.040 0.000 0.000 0.000 0.380 0.055 0.007 0.007 0.002 0.000 0.000 0.000 0.000 0.000 0.837 0.683 0.667 0.667 0.667 0.400 0.020 0.000 0.000 0.000 0.683 0.667 0.667 0.667 0.667 0.000 0.000 0.000 0.000 0.000 ￼￼￼￼￼￼Accuracy level ε F12 (20D) SR F11 (5D) F12 (5D) F11 (10D) F12 (10D) ￼￼￼￼￼￼￼￼￼￼￼PR SR PR SR PR SR PR SR PR ￼1.0E-01 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.380 1.0E-02 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-03 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-04 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 1.0E-05 ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ 0.000 0.697 0.667 0.667 0.667 0.667 0.000 0.000 0.000 0.000 0.000 0.567 0.425 0.280 0.115 0.047 0.080 0.000 0.000 0.000 0.000 0.517 0.250 0.200 0.173 0.170 0.080 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.502 0.013 0.000 0.000 0.000 ￼￼</Text>
        </Document>
        <Document ID="15">
            <Title>Pitfalls and Problems</Title>
            <Text>In the beginning of the project, after translating each function into the R programming language and in order to start testing each one of them, some data was missing. The algorithm runs with some parameters that had to be given by the CEC. So it was necessary to contact the organisation and ask for the files including the benchmarking tool. Also, Dr. Fieldsend was contacted in order to have a better understanding of the general structure of his algorithm. Once the data was completed, the test phase started, trying to get the same results as the original one, but this was not possible at first instance because the output was not generalised and most of the data generated inside the algorithm were random values to be evaluated.

During the implementation of the NMMSO.R algorithm, some problems regarding the language structure came up immediately. After some first test cases, it was discovered how different Matlab and R work with matrices and vectors. In the case of Matlab, every time a value wanted to be added for an inexistent index, this was created automatically and added to the data structure, where no row was existing before, instead of throwing the common “index out of bounds” error. First, it was necessary to compare all these possible behaviors in Matlab so a general way to attack them could be implemented. After few tries, two function-files were created. “add_col.R”, as its name says, is in charge to add a new column into a structure. It is just necessary to specify the original structure where it will be added to, the index and the new object containing the information to add. This function considers the cases, whether the new object is a vector, a matrix or just a single value, so the original matrix could be modified and returned as desired. “add_row.R”, on the other hand, simply imitates the behavior of “add_col.R” but as a transpose matrix in order to add the rows.

Once the data was completed in order to run the algorithm and get at least a similar output as the original version, the CEC benchmarking tool was intended to be used for testing. Thanks to the CEC organisation, these tools were provided in C++, Java and Matlab to check which one could be the best implementation for this case. After some trials with the C++ version, some issues regarding its implementation and missing documentation lead the development team to re-write the complete benchmarking tool in the R programming language, in order to avoid mixing code and be sure to get the results expected since the beginning of this project. Also, it was thought in a way to supply the research community with a new implementation of this tool for future projects.

After solving the previous issues and starting to run the whole algorithm, a memory problem came up. Mainly because of the continuous problem between R and Matlab, this was caused, because one matrix, originally containing only integer values, was added a float value in a certain point within the process. R automatically cloned the integer matrix, creating it with a float type in order to continue without errors for every iteration. In the end having an unnecessary number of matrices allocated in memory, making impossible the computation of the algorithm with a size of 7.9 GB during the iteration 700 out of 50,000 (depending on the function to be evaluated in CEC benchmark suite, the total number of iterations would vary). Even if this issue was solved only by changing the original matrix to contain float values since the beginning, it caused several problems and time consumption during the testing phase.

Finally, after completing and fixing the R implementation, and trying to test it with the real number of iterations for each of the twenty CEC benchmarking functions, a bigger computation power was needed. Having only students’ computers for development, a bigger source of computation power was necessary. With the help of the R library “BatchJobs” together with the Palma Cluster, property of the Westfälische Wilhelms-Universität located in Münster, Germany, it was possible to run the algorithm in different batch jobs within the cluster and just printing output files in order to analyse the data in the end.
</Text>
        </Document>
        <Document ID="26">
            <Title>Benchmarking</Title>
        </Document>
        <Document ID="16">
            <Title>Benchmark and Comparison</Title>
            <Text>
To compare nmmso.R with the original NMMSO the CEC test cases were used to run the same benchmarks as in the original submission [@fieldsend_2014]. There 4 different Ratios were used to measure the performance of certain algorithms. Three of those measures (Peak Ratio, Success Ratio and Convergence Speed) have been introduced in [@li_2013, pp. 6-7] to create a common point of comparison. The fourth ratio is special for the nmmso algorithm since it tracks the number of swarms over the iterations of the algorithm. Nmmso.R uses the same measures to reach the highest comparability possible. All of the following measures were taken over several runs and contain all results which could be received until the deadline of this paper. The number of runs in total for each function is stated in the first three tables and matches in the graph.

The first measure used is the Success Ratio (SR). The Success Ratio is defined as the percentage of successful runs (runs that found all global optima) over all runs [@li_2013, p. 7]. As for the other ratios this measure was taken over several independent runs and collectively evaluated. The taken measures for the Success Ratio can be found in Table 3. 
$$\frac{successful\ runs}{NR} = SR $$ 
Here $NR$ denotes the Number of runs done to reach this measure.
\newline
```{r, SR, echo=FALSE}
rm(list = ls())
# remove scientific notation
library(plyr)
options(scipen=999)
# calculate sr
source("../R/cec_2015_problem_data.R")
# calculate sr
library(pander)
panderOptions('table.alignment.default', function(df) ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('digits', 2)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)

names &lt;- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")
sr &lt;- as.data.frame(matrix(0,nrow=20,ncol=6))
mean &lt;- as.data.frame(matrix(0,nrow=20,ncol=6))
sd &lt;- as.data.frame(matrix(0,nrow=20,ncol=6))
for(i in 1:20){
	filenames.output &lt;- list.files(full.names = TRUE, path='../output/', pattern=paste("(^",i,")_.*\\output.txt$", sep=""))
	if(length(filenames.output) != 0){
		results &lt;- lapply(filenames.output, read.table)
		names(results) &lt;- paste0('run',seq_along(results))
		c_list &lt;- ldply(results)
		sr[i,] &lt;-c(apply(c_list[,2:6], 2, function(x) length(which(x &lt; gens[i]))/length(x)), length(unique(c_list$.id)))
		mean[i, ] &lt;- c(apply(c_list[,2:6], 2, function(x) mean(x)), length(unique(c_list$.id)))
		sd[i,] &lt;-c(apply(c_list[,2:6], 2, function(x) sd(x)), length(unique(c_list$.id)))
	}
}
rownames(sr) = names
rownames(mean) = names
rownames(sd) = names
sr &lt;- rename(sr, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))
mean &lt;- rename(mean, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))
sd &lt;- rename(sd, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))
set.caption('Success Ratio over given runs (Measure of share of runs which found all global optima)')
pander(sr)

```

The second measure introduced by the CEC committee and also used by Dr. Fieldsend is the Convergence Rate. The Convergence Rate (CR) measures the needed evaluations per Accuracy and Function to find all global optima [@li_2013, p.7]. This measure takes the mean of evaluations over all runs. The results of this measure can be found in Table 4.

$$\frac{\sum\nolimits_{n=1}^{NR} evals_{n}}{NR} = CR$$ 
In this measure, $evals$ denotes the number of evaluations done. 
\newline
```{r, Mean, echo=FALSE}
set.caption('Convergence Rates over given runs (Mean of evaluations needed to find all global optima, if all optima have never been found the maximum allowed evaluations for that function were taken.)')
pander(mean)
```

The third measure is the Peak Ratio (PR). It measures the share of found global optima over all runs [@li_2013, p.7]. The results of this evaluation can be found in Table 5.

$$\frac{\sum\nolimits_{n=1}^{NR} NOF_{n}}{NKO * NR} = PR$$
\newline
In this measure $NOF$ denotes the number of found optima per run and $NKO$ the number of known optima for the function. 
\newline
```{r, MPR, echo=FALSE}
source("../R/cec_2015_problem_data.R")
names &lt;- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")
mpr &lt;- as.data.frame(matrix(0,nrow=20,ncol=6))
for(i in 1:20){
	filenames.output &lt;- list.files(full.names = TRUE, path='../output/', pattern=paste("(^",i,")_.*\\clist.txt$", sep=""))
	if(length(filenames.output) != 0){
		results &lt;- lapply(filenames.output, read.table)
		max &lt;- lapply(results, function (x) sapply(x, max))
		names(max) &lt;- paste0('run',seq_along(max))
		c_list &lt;- ldply(max)
		mpr[i,] &lt;-c(apply(c_list[,2:6], 2, function(x) mean(x)/nopt[i]), length(unique(c_list$.id)))
	}
}
rownames(mpr) = names
mpr &lt;- rename(mpr, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))
set.caption('Peak Ratio over given runs (Share of found global optima over all runs)')
pander(mpr)

```

As a fourth measure, which wasn't introduced by the CEC committee, but used in the original nmmso implementation [@fieldsend_2014], the Number of Swarms was chosen. Since this is a continuous measure and, therefore no calculation is needed, this measure is pictured as graphs. The graphs can be found in Figure 1. They show the development of $number$ $of$ $swarms$ kept by nmmso.R over all iterations. Important to notice here is that $iterations$ is different from the $evaluations$ referenced in the other measures. Iterations are calls to start single runs of nmmso.R and, therefore, are different from the evaluations taken within the program.


![plot of chunk trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs. These plots contain all runs pictured in the other tables which were done for the benchmarking of this paper.](figure/trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs.-1.pdf) 


```{r, trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs., echo = FALSE, dpi = 250, fig.ext='pdf', keep='all'}
# rm(list = ls())
# source("../R/cec_2015_problem_data.R")
# library(plyr)
# library(ggplot2)
# library(ggthemes)
# library(gridExtra)
# library(grid)

# plots = list()
# for(i in 1:20){
# 	results &lt;- NA
# 	filenames.swarms &lt;- list.files(full.names = TRUE, path='../output', pattern=paste("(^",i,")(_.*)([[:digit:]].txt)$", sep=""))

# 	results &lt;- lapply(filenames.swarms, read.table)
# 	names(results) &lt;- paste0('run',seq_along(results))
# 	results &lt;- ldply(results)

# 	results &lt;- rename(results, c("V1" = "iterations", "V2" = "swarms"))

# 	#results &lt;- transform(results, run = sprintf('run%d', run)) 

# 	p1 = ggplot(results, aes(x = iterations, y = swarms, group=.id, colour="grey"))
# 	p1 = p1 + ggtitle(paste0("F",i)) + geom_line() + theme_tufte() + theme(legend.position = "none")
# 	p1 = p1 + stat_summary(fun.y = mean, geom="line", colour="black", aes(group = 1))
# 	plots[[i]] = p1 + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(plot.margin=unit(c(0,0,0,0),"mm"))
# }
# do.call("grid.arrange", c(plots, ncol = 4))
```

When comparing those measures with the ones given in the original paper [@fieldsend_2014], it can be seen that the reimplementation nmmso.R is an overall good resemblance of the original algorithm. The three CEC measures are close to the original taken measures and the trend curves for the number of kept swarms have similar trends. 

The biggest differences between the benchmarking results of the two implementations can be seen in the general results of function 14, 15, 16 and 18, as well as in the number of created swarms for the n-dimensional functions: 

(1) Function 14 and 15 have a $Success\ Ratio$ of $1$ aswell as $Peak$ $Ratio$ of one $1$ for all accuracy levels. Additionally nmmso.R sometimes found all global optima for Function 18.  In contradiction of all three function almost never result in the finding of global optima in the evaluation of the original implementation. Only at the lowest accuracy, the original implementation is able to find all global optima for Function 14 [@fieldsend_2014, p.16].  It is hard to say if this difference is equal to an error in the implementation of nmmso.R or if an error in the original implementation was fixed. Also, this could be a difference in the reimplementation of the CEC Benchmarking Tool. Nevertheless, this is an interesting point of discussion and worth evaluating.

(2) nmmso.R performs noticeable worse for Function 16 than for the original function. While nmmso.R has a $Peak$ $Ratio$ of $0.01$ for an accuracy of $0.1$ and $0$ for all others, the original implementation reaches a $Peak$ $Ratio$ of around $0.6$ for all accuracies. This might be an implementation error in the CEC Benchmarking Tool and not in nmmso.R since it is so significantly worse that it is unlikely that this difference would only occur in one test function.

(3) Almost all algorithm runs on high-dimensional functions (F12-F20) result in a high number of swarms while all other results regarding this functions are comparable to the original results. This difference becomes very clear in the case of Functions 17-20. In the paper addressing the original implementation the x-axis rank from 0-40,000 iterations while for the reimplementation limit of 4,000 for Function 17, of 20,000 for Function 18, 7,500 for Function 19 and of 30,000 for Function 20 is enough to show all data sets. This is connected to the creation of much more swarms, which leads to an earlier depletion of the maximum allowed number of evaluations.

Additionally, a fifth measure was introduced which denotes the runtime of nmmso.R for the single functions. These times were taken on the ZIVHPC, a scientific High Perfomance Computing Cluster by Westfälische Wilhelms-Universität Münster. Since the nmmso.R is a strictly sequential algorithm the runtimes for single runs will be comparable on common computers. The ZIVHPC was only used to parallelise the single runs. Even though this measure widely varies depending on the computer's architecture it can show the different complexity of all 20 functions. This measure was taken separately on 10 runs of each function. 

```{r, time consumption, echo=FALSE}
names &lt;- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")
times &lt;- as.data.frame(matrix(0, nrow = 20, ncol = 2))
for(i in 1:20){
	filenames.times &lt;- list.files(full.names = TRUE, path='../output', pattern=paste("(^",i,")_.*\\_time.txt$", sep=""))
	if(length(filenames.times) != 0){
		results &lt;- lapply(filenames.times, read.table)
		time &lt;- ldply(results)
		mean &lt;- mean(time$V3)
		sd &lt;- sd(time$V3)
		times[i,] &lt;- c(mean, sd)
	}
}
rownames(times) = names
times &lt;- rename(times, c("V1" = "Mean", "V2" = "Standard Deviation"))

set.caption('Taken time of nmmso.R for all 20 functions over 10 runs. All times are in seconds.')
pander(times)
```</Text>
        </Document>
        <Document ID="22">
            <Title>ihaka_98</Title>
            <Text>Abstract R began as an experiment in trying to use the meth- ods of Lisp implementors to build a small testbed which could be used to trial some ideas on how a statistical environment might be built. Early on, the decision was made to use an S-like syntax. Once that decision was made, the move toward being more and more like S has been irresistible. R has now outgrown its origins and its development is now a collaborative effort undertaken using the Internet to exchange ideas and distribute the results. The focus is now on how the initial experiment can be turned into a viable piece of free software. This paper reviews the past and present status of R and takes a brief look at where future development might lead. 1 Genesis A long time ago I discovered a wonderful book by Hal Abelson and Gerald Sussman called The Structure and Interpretation of Computer Programs. The book aims to introduce engineering students to computing using the Scheme programming language. It presents a wonderful view of programming; investigating a wide variety of in- teresting and practical examples, and even showing how a language like Scheme can be implemented. At about the same I obtained access to one of the first releases of Rick Becker and John Chambers’ New S language. I remember noticing both similarities and differences between S and Scheme. In particular, I re- member that one day I wanted to show Alan Zaslavsky how you could use lexical scope to obtain own variables. I didn’t have a copy of Scheme handy, so I tried to show him using S. My demonstration failed because of the dif- ferences in the scoping rules of S and Scheme. It left me thinking that there were useful additions which could be made to S. Rather later, Robert Gentleman and I became col- leagues at The University of Auckland. We both had an interest in statistical computing and saw a common need for a better software environment in our Macin- tosh teaching laboratory. We saw no suitable commer- cial environment and we began to experiment to see what might be involved in developing one ourselves. It seemed most natural to start our investigation by working with a small Scheme-like interpreter. Because it was clear that we would probably need to make sub- stantial internal changes to the interpreter we decided to write our own, rather than adopt one the many free Scheme interpreters available at the time. This is not quite as daunting a task as it might seem. The process is well mapped out in books such as that of Abelson- Sussman and that of Kamin. Having access to the source code of a number Scheme interpreters also helped with some of the concrete implementation details. Our initial interpreter consisted of about 1000 lines of C code and provided a good deal of the language func- tionality found in the present version of R. To make the interpreter useful, we had to add data structures to sup- port statistical work and to choose a user interface. We wanted a command driven interface and, since we were both very familiar with S, it seemed natural to use an S-like syntax. This decision, more than anything else, has driven the direction that R development has taken. As noted above, there are quite strong similarities between Scheme and S, and the adoption of the S syntax for our interpreter produced something which “felt” remarkably close to S. Having taking this first step we found ourselves adopting more and more features from S. Despite the similarity between R and S, there remain number of key differences. The two fundamental differ- R : Past and Future History A Draft of a Paper for Interface ’98 Ross Ihaka Statistics Department The University of Auckland Auckland, New Zealand &gt; total &lt;- 10 &gt; make.counter &lt;- + + + + +} &gt; counter &lt;- make.counter() &gt; counter() [1] 1 &gt; counter() [1] 2 &gt; counter() [1] 3 Figure 1: A simple function demonstrating how the scoping rules in R differ from those of S. function(total = 0) function() { total &lt;&lt;- total + 1 total ences result from R’s Scheme heritage. • Memory Management: In R, we allocate a fixed amount of memory at startup and manage it with an on-the-fly garbage collector. This means that there is very little heap growth and as a result there are fewer paging problems than are seen in S. • Scoping: In S, variables in functions are either lo- cal or global. In R we allow functions to access to the variables which were in effect when the function was defined; an idea which dates back to Algol 60 and found in Scheme and other lexically scoped lan- guages. Consider the function definition in figure 1. The function make.counter returns a value which is itself a function. This “inner function” increments the value of the variable total, and then returns the value of that variable. In S, the variable being manipulated is global. In R, it is the one which is in effect when the function is defined; i.e. it is the argument to make.counter. The effect is to create a variable which only the inner function can see and manipulate. Generally, the scoping rules used in R have met with approval because they promote a very clean pro- gramming style. We have retained them despite the fact that they complicate the implementation of the interpreter. The two differences noted above are of a very basic na- ture. In addition, we have experimented with a number of other features in R. A good deal of the experimenta- tion has been with the graphics system (which is quite similar to that of S). Here is a brief summary of some of these experiments. • Colour Model: R uses a device independent 24-bit model for colour graphics. Colours can be specified in a number of ways. 1. By specifying the levels of red, green and blue primaries which make up the Colour. For ex- ample, the string "#FFFF00" indicates full in- tensity for red and green with no blue; produc- ing yellow. 2. By giving a colour name. R uses the colour naming system of the X Window Sys- tem to provide about 650 standard colour names, ranging from the plain "red", "green" and "blue" to the more exotic "light goldenrod", and "medium orchid 4". 3. As a index into a user settable colour table. This provides compatibility with the S graphics system. • Line Texture Description: Line textures can also be specified in a flexible fashion. The specification can be: 1. A texture name (e.g. "dotted"). 2. A string containing the lengths for the pen up/down segments which compose a line. For example, the specification "52" indicates 5 points (or pixels) with “pen down” followed by 2 with “pen up”, with the pattern replicated for the length of the line. 3. An index into a fixed set of line types, again providing compatibility with S. • Mathematical Annotation: Paul Murrell and I have been working on a simple way of producing 2 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0.0 0.1 0.2 0.3 0.4 0.5 Frequency λ Figure 2: A plot of the periodogram of a white-noise mathematical annotation in plots. Mathematical annotation is produced by specifying an unevalu- ated R expression instead of a character string. For example, expression(x^2+1) can be used to produce the mathematical expression x2 + 1 as annotation in a plot. The annotation system is relatively simple, and not designed to have the full capabilities of a system such as TEX. Even so, it can produce quite nice results. Figure 2 shows a simple example of a time series periodogram plot produced in R. The plot was produced with a single R command which used expression to describe the labels. • Flexible Plot Layouts: A part of his PhD re- search, Paul Murrell has been looking at a scheme for specifying plot layouts. The scheme provides a simple way of specifying how the surface of the graphs device should be divided up into a number time series, showing the use of mathematical annotation. of rectangular plotting regions. The regions can be constrained in a variety of ways. Paul’s original work was in Lisp, but he has implemented a use- ful subset to R. These graphical experiments were carried out at Auck- land, but others have also bound R to be an environment which can be used as a base for experimentation. • Compilation: Luke Tierney has performed some experiments to see what kind of performance gains could be obtained by using byte-code compilation of R. His experiments indicated that a speed-up by a factor of 20 might be possible for some interpreted code. As yet, the internal data structures in R are probably not stable enough to make it worthwhile to follow up on this work. • WWW Interface: Jeff Banfield has developed RWeb, which is a WWW based interface to R. • Tcl/Tk Interface: Very recently Balasubrama- nian Narasimhan has begun looking into how Tcl/Tk might be used to add a fully graphical user interface to R. 3 log I(T)(λ) 10 XX –4 –3 –2 –1 0 1 2 2 A Free Software Project 2.1 A Brief History The initial work on R by Robert Gentleman and I pro- duced what looked like a potentially useful piece of soft- ware and we began preparing it for use in our teaching laboratory. We were heartened enough by our progress to place some binary copies of R at Statlib and make a small announcement on the s–news mailing list in Au- gust of 1993. A number of people picked up our binaries and of- fered feedback. The most persistent of these was Martin Ma ̈chler of ETH Zurich, who encouraged us to release the R source code as “free software”. We had some initial doubts about doing this, but Mar- tin’s arguments were persuasive, and we agreed to make the source code available by ftp under the terms of the Free Software Foundation’s GNU general license. This happened in June of 1995. At this point, the development of R was a relatively closed process. Robert and I (soon joined by Martin) would get bug reports by e-mail and from time-to-time release updated versions of R. We quickly noticed that there was no real forum for users to discuss R with each other and so we began maintaining a small mailing list. As interest in R grew (mostly by word of mouth) it became clear that manually maintaining the mailing list was not an effective option. Worse than that, at Auck- land we were paying for e-mail, and the cost was be- ginning to become noticeable. Eventually Martin vol- unteered the use of facilities at ETH Zurich to establish automated mailing lists to carry discussions about R and R development. In March of 1996 the r–testers mailing list was started. Roughly a year later this was replaced with three newsgroups: r–announce, r–help and r–devel. As R developed and people began porting applications to it, it became clear that we needed a better distribu- tion mechanism. After some discussion it was decided a formal archive mechanism was desirable. Kurt Hornik of TU Wien took on the task of establishing the archive. In addition to the master site in Austria there are a number of mirror sites, including StatLib. With the introduction of the mailing lists, develop- ment on R accelerated. This was partly because we ob- tained many more reports and suggestions and partly because we also began to receive patches and code con- tributions. The contributions ranged from fixes for typos through to changes which provided substantial increases in functionality and performance. The level of contribution was such that Robert, Mar- tin and I couldn’t always make changes at a rate which was satisfactory to those asking for changes. As a re- sult, in mid-1997 we established a larger “core group” who can make changes to the source code CVS archive. This group currently consists of: Doug Bates, Peter Dalgaard, Robert Gentleman, Kurt Hornik, Ross Ihaka, Friedrich Leisch, Thomas Lumley, Martin Ma ̈chler, Paul Murrell, Heiner Schwarte, and Luke Tierney. Since all work on R is strictly of a voluntary nature, the organisation is very loose, with members contributing when and as they can. 2.2 Contributors When Robert and I started work on R, we were hopeful that we might be able to produce something we could use to teach our introductory data analysis courses. Had we continued to work strictly on own it is likely that this is precisely what we would have achieved. The decision to make R free software has enabled us to set rather higher goals, because it has given us access to a large pool of very talented individuals who have been willing to invest significant effort in the project. Indeed, one of the very best things about having worked on R has been the chance to work with such a great group of people. In addition to the core group listed above, I would like to acknowledge the following individuals who have made significant contributions to R. Valerio Aimale, Ben Bolker, John Chambers, Simon Davies, Paul Gilbert, Arne Kovac, Philippe Lambert, Alan Lee, Jim Lindsey, Patrick Lindsey, Mike Meyer, Martyn Plummer, Anthony Rossini, Bill Venables, Gregory Warnes, and mward@wolf.hip.berkeley.edu (I apologise for omissions here. Our record keeping has not been all that it could be). In addition a host of other individuals have made con- tributions. 2.3 Present Status R is still under active development and there is still some work needed before it can be considered ready for widespread use. In particular, some changes will be required to support moderate to large-sized data sets. 4 More importantly, there is an almost complete lack of introductory documentation, although much of what has been written about S directly applicable to R. Despite this, it seems that R is beginning to reach the point where it is stable enough for regular use (at least under Unix). I am hopeful that during the next year we can release a complete R version 1.0 package as part of the Free Software Foundation’s GNU suite of software. 3 The Future 3.1 R It is the present aim of the R project to produce a free implementation of something “close to” version 3 of the S language and to provide ongoing support and mainte- nance for the resulting software. Some members of the R core have proposed that future developments in S ver- sion 4 should be also tracked. At this point it is unclear whether this will happen. One development which would help R a good deal would be the development of an integrated graphical user interface. Some initial work has begun on this and I be- lieve that it is something which will come quite quickly. My personal future interest in R is mainly as a user. Given the investment I have made in it, I hope that I will be able to get substantial use out of R for statistical work and teaching. 3.2 Related Work Working on R has shown me that there a number of interesting questions related to building statistical soft- ware. My own conclusion has been that it is important to pursue efficiency issues, and in particular, speed. As noted in section 1, Luke Tierney performed some experiments with R to see what kind of speed increase could be obtained using byte-code compilation; the in- dications were that a speedup by a factor of 20 might be possible for some computations. There is other evidence that a factor of 100 (roughly the speed of unoptimised C) might be possible with com- pilation to native machine code. With this level of per- formance, there would be no need for any foreign func- tion interface and all computations could take place in a single language environment. I am intrigued by what such an environment might offer. An increase in performance of this magnitude is likely to produce a qualitative change in the use it gets puts to. The difficulty is that the creation of such a compiled environment requires the hand of an expert in compila- tion. There is a real problem in finding such an expert who is also aware of the type of problems which statis- ticians handle. 4 Acknowledgements It goes without saying that R would not exist without the pioneering work of John Chambers and his AT&amp;T collaborators. John has changed the way that many of us think about statistical computing and the fact that R has evolved to resemble S as closely as it does is is a testimony to the extent that people enjoy doing data analysis with S. The Free Software Movement (or movements perhaps) has been another major source or ideas and influence on R. I do my development work on a workstation which runs the FreeBSD operating system and is equipped with a rich set of development tools from the Free Software Foundation. Hopefully, R represents some pay back to the free software community for what they have provided to me and the other R developers. Finally, I’d like to acknowledge my partner-in-crime; Robert Gentleman. During our work on R we have prac- tically lived in one another’s back pockets. It speaks vol- umes that, not only are we still on speaking terms, but we still go out for beer together on Friday evenings. References Abelson, H. and G. J. Sussman, with J. Sussman (1985). Structure and Interpretation of Computer Programs. Cambridge MA: MIT Press. Becker, R. A., Chambers, J. M. and Wilks, A. R. (1986). The new S Language: A programming environ- ment for data analysis and graphics. Pacific Grove, CA: Wadsworth &amp; Brooks/Cole. Chambers, J. M. and T. J. Hastie, Eds. (1991). Sta- tistical Models in S. Pacific Grove, CA: Wadsworth &amp; Brooks/Cole. R. Gentleman and R. Ihaka (1997). “The R language”, In Proceedings of the 28th Symposium on the Interface, L. Billard and N. Fisher Eds. The Interface Foundation of North America. Ihaka, R. and R. Gentleman (1996). “R: A language for data analysis and graphics,” Journal of Computational and Graphical Statistics, 5, 299–314. Kamin, S. N. (1990). Programming languages. Addison Wesley. 5</Text>
        </Document>
        <Document ID="27">
            <Title>Todo</Title>
            <Text>	•	1. Introduction -&gt; add something about the done evaluation of algorithm
	•	2.1 General Function -&gt; don't write as much about history of evolutionary algorithms, but more about multimodal optimaziation
	•	2.2 CEC -&gt; show more that implementation of benchmarking suite was own work, show we tested it and "proved" that it is working as described. Also show the problems that occured here.
	•	4. Benchmark -&gt; describe the definition of the single measures.
	•	Delete list of figures
	•	Give examples for paramaters, basically and explanation on how to use nmmso
	•	self-contained description of tables and graphs
	•	maybe introduce grayscale table to better visualize the measures?
	•	show runtimes of algorithm
	•	Right-align tables
	•	Enumeration for equations?
</Text>
        </Document>
        <Document ID="12">
            <Title>General Function</Title>
            <Text>The starting point of the project was the paper provided by Dr. Jonathan E. Fieldsend [@fieldsend_2014] on the Niching Migratory Multi-Swarm Optimiser (NMMSO) algorithm. NMMSO is a multi-modal optimiser which relies heavily on multiple swarms that are generated on the landscape of a function in order to find the global optima. It is build around three main pillars: 
(1) dynamic in the numbers of dimensions, 
(2) self-adaptive without any special preparation and 
(3) exploitative local search to quickly find peak estimates [@fieldsend_2014, p. 1]. 

Multi-modal optimisation in general, does not differ very much from well known and widely discussed single-objective optimisation, but but unlike it, the goal of the algorithms in the multi-modal perspective is not to find just one single optimising point but all possible points [@fieldsend_2014, p. 1].  In order to do so, many early multi-modal optimisation algorithms needed defined parameters [@fieldsend_2014, p.1].

Newer algorithms fall in the field of self-tuning and try to use different mathematical paradigms like nearest-best clustering with covariance matrices [@preuss_2012] and strategies like storing the so far best found global optima estimators to provide them as parameters for new optimisation runs [@epitropakis_2013]. Contradictory to that NMMSO goes the way of many early algorithms and uses the swarm strategy in order to find which store their current [@fieldsend_2014]. 

In order to do so, NMMSO follow a strict structure, which can be seen in the following pseudo-code

	nmmso (max_evals, tol, n, max_inc, c_1, c_2, omega)
		S: initialise_swarm(1)
		evaluations := 1
		while evaluations &lt; max_evals:
			while flagged_swarms(S) == true:
				{S, m} := attempt_merge(S, n, tol)
				evals := evals + m
			S := increment(S, n, max_inc, c_1, c_2, omega)
			evals := evals + min(|S|, max_inc)
			{S, k} := attempt_separation(S, tol)
			evals := evals + k
			S := add_new_swarm(S)
			evals := evals + 1
		{X*, Y*} := extract_gbest(S)
		return X*,Y*

This structure wasn't modified during the reimplementation of  NMMSO to keep comparability and the possibility to fix bugs at a high level. The only newly introduced setting was the possibility to modify c_1, c_2 and omega as parameters from the outside. In the original version those parameters are part of the program code. All other variables have been set to the variables seen in the following table. While variables like maximal amount of evaluations are dependent on the used test function and have been explicitly stated in the test function by the CEC (Section 3.1) other values have been chosen by Dr. Fieldsend and were used for the reimplementation [@fieldsend_2014, p. 2-3]. Those values can be found in the following table:

           		used value 	
-----------	 	--------------
evaluations		0			
max_inc			100			
tol				10^-6		
c_1				2.0         
c_2				2.0	        
omega			0.1       	
---------  		--------------   
Table: Values used for all program evaluations.   

</Text>
        </Document>
        <Document ID="23">
            <Title>running_up_those_hills_2014</Title>
            <Text>Running Up Those Hills: Multi-Modal Search with the Niching Migratory Multi-Swarm Optimiser Jonathan E. Fieldsend College of Engineering, Mathematics and Physical Sciences University of Exeter Exeter, UK, EX4 4QF Email: J.E.Fieldsend@exeter.ac.uk Abstract—We present a new multi-modal evolutionary opti- miser, the niching migratory multi-swarm optimiser (NMMSO), which dynamically manages many particle swarms. These sub- swarms are concerned with optimising separate local modes, and employ measures to allow swarm elements to migrate away from their parent swarm if they are identified as being in the vicinity of a separate peak, and to merge swarms together if they are identified as being concerned with the same peak. We employ coarse peak identification to facilitate the mode identification required. Swarm members are not constrained to particular sub- regions of the parameter space, however members are initialised in the vicinity of a swarm’s local mode estimate. NMMSO is shown to cope with a range of problem types, and to produce results competitive with the state-of-the-art on the CEC 2013 multi-modal optimisation competition test problems, providing new benchmark results in the field. I. INTRODUCTION An effective multi-modal optimiser embodies a number of traits: (1) it is dynamic in the number of modes it maintains and returns, enabling it to be applied to problems with few or many modes; (2) it is self-adaptive, with few meta-parameters, enabling a wide range of problem types to be tackled without prior tuning; (3) it incorporates exploitative local search, enabling it to rapidly hone the peak estimates it is maintaining. Although the algorithm genus may vary (genetic algorithm, evolutionary strategy, particle swarm, differential evolution, etc.), it is these three properties which all the best-performing algorithms in the CEC 2013 competition contain [1]. Here we present a new optimiser which embeds the three properties listed above, and utilises multiple particle swarm optimisers to rapidly climb peaks in the search landscape. The swarms do not operate in isolation: elements can migrate away from their parent swarm if they are judged to have discovered a separate peak. Swarms may also be merged if they are identified as converging on the same peak. Additionally, swarm members are not constricted to movements within a local region of design space. The paper proceeds are follows. In Section II we describe the general multi-modal optimisation problem, and highlight the difficulties that confront optimisers in this arena. In Section III we briefly discuss some of the popular evolutionary com- putation approaches to multi-modal optimisation. In Section IV we introduce the niching migratory multi-swarm optimiser (NMMSO), describing its properties and relationship to other approaches. This is followed by empirical results on the CEC 2013 competition test problems. The paper ends with a discussion in Section VI. II. MULTI-MODAL OPTIMISATION The general aim in multi-modal optimisation is similar to standard uni-objective optimisation, that is, given a legal search domain X, without loss of generality, we seek to maximise f(x), x ∈ X, given any equality and inequality constraints. In the case of a multi-modal problem however, we seek not simply to discover a single design x which maximises f(x) given the constraints, but all x∗ ∈ X which obtain the maximum possible function response, but which inhabit isolated peak regions. That is, the mapped objective values in the immediate region of an x∗ are all equal or lower than f(x∗). Local optima (local modes/peaks) in contrast are locations which are surrounded in the immediate vicinity with less ‘fit’ solutions (lower responses from f(·)), but which do not themselves have the highest possible fitness obtainable. Local regions around a peak are often called niches. There are many reasons that the problem owner may wish multiple mode solutions to be discovered rather than a single ‘best’ solution. By discovering a range of different designs which are operationally equivalent insight into the problem domain may be extracted. Also, it may transpire that some designs are not machinable – i.e., X is misspecified, and therefore a range of solutions mitigates against this. Finally f(·) may be in error in certain regions, therefore a wide range of good solutions can be helpful if the ‘best’ design does not perform as emulated (it is useful to have local optima, not just global optima stored – as there is no guarantee that all the global optima under f(·) are not in error). III. EVOLUTIONARY MULTI-MODAL OPTIMISERS One of the earliest approaches to evolutionary multi-modal optimisation is derived from the fitness sharing concept, first introduced in [2]. This was later refined as a means to partition a genetic algorithm search population into different subpopulations based on their fitness [3]. An overview of these general niching ideas is presented in [4]. As highlighted in e.g. [5], many early multi-modal optimis- ers tended to be highly parametrised, relying on well-chosen values to perform well (for instance specifying a priori what the niche width should be set as, or how many modes to search for). However, a recent design trend of the most effective multi-modal optimisers is to make them to a large degree ‘self- tuning’. The current state-of-the-art (based upon the results of the CEC 2013 competition in the field) rely on a range of different technologies and heuristics to maintain, search for and exploit mode estimates. The optimiser that was the best performing overall, proposed in [6], utilises the covariance matrix adapta- tion evolution strategy (CMA-ES) of [7]. Rather than selecting the restart location at random, [6] used nearest-better clustering to partition a search population into sub groups concerned with different modes. This is facilitated by fitting a spanning tree on the population, linking all population members to their nearest neighbour (in design space) which is better performing under f(·), and disconnecting the longest edges (thus assuming that the best search points on different peaks are likely to be further away from each other than neighbours on the same peak). This leads to another property of this approach – it is dynamic in the number of modes it maintains and returns (although limited to a maximum number, set a priori). The second best performing multi-modal optimiser [8] also has dynamic mode maintenance, storing an external dynamic archive of estimated mode locations which supports a reinitialisation mechanism, along with an adaptive control parameter technique for the differential evolution algorithm driving the search. The third best was the standard CMA-ES algorithm. Finally, the fourth ranked algorithm proposed by [9] uses an external memory to store the current global optima, along with an adaptive niche radius to mitigate the effect of setting this parameter a priori to a value which may not be appropriate to the problem at hand. A mesh of solutions is exploited, with a combination method that generates solutions in the direct of nearest (estimated) global optima. The published ranking of these algorithms is derived from their average performance on the twenty problem formulations used in the CEC 2013 benchmark suite, averaged across five different accuracy levels for fixed numbers of function evaluations. Given different test problems, and/or a different number of permitted function evaluations and accuracy levels, there may of course be a different ranking obtained. The top ranked algorithms do however possess a number of similar characteristics which would seem to describe an effective multi-modal optimiser, namely: self-adaption of search pa- rameters, dynamic mode maintenance, and exploitative local search. Here we leverage and develop these ideas, and build on aspects of our recent work in the area, which use coarse mode region identification in conjunction with local surrogates and hill-climbers [10], [11]. IV. USING MULTIPLE SWARMS Particle swarm optimisation PSO has gained widespread popularity since its introduction in [12] for the optimisation of continuous non-linear functions, due to its rapid con- vergence properties on a wide range of problems, and its relative simplicity (and therefore ease of implementation). A fixed population of solutions is used, where each solution (or particle) is represented by a point in D-dimensional design space. The ith particle is commonly represented as xi = (xi,1 , xi,2 , . . . xi,D ), and its performance is evaluated on a given problem and stored. Each particle maintains knowledge of its best previous evaluated position, represented as pi (commonly referred to as ‘pbest’), and also has knowledge of the single best solution found so far in some defined neighbourhood, gi (commonly referred to as ‘gbest’). Often this is with respect to a global neighbourhood (all particles are considered), however other neighbourhood definitions can also be used. The rate of position change of a particle then depends upon its previous personal best position and the neighbourhood best, and its previous velocity. For particle i this velocity is vi = (vi,1 , . . . , vi,D ), typically initialised at random in X . The general algorithm for the adjustment of these velocities is: vi,j := ωvi,j + c1r1(bi,j − xi,j) + c2r2(gi,j − xi,j), (1) and the position is updated as: xi,j:=xi,j+χvi,j, j=1,...,D, (2) where ω, c1, c2, χ ≥ 0. ω is the inertia of a particle, c1 and c2 are constraints on the velocity toward local best and neighbourhood best - referred to as the cognitive and social learning factors respectively, χ is a constraint on the overall shift in position, and r1,r2 ∼ U(0,1). In [12], the final model presented has w and χ set at 1.0 and c1 and c2 set at 2.0. As discussed in [13], in this classical form of PSO each particle xi is flown toward pi, gi and vi. This, in effect, means that a hypercuboid is generated in design space, the bounds of which are the sum of the distances from xi to the other three points (weighted by the appropriate multiplier constants from (1) and (2)). The length of the jth dimension of the containing hypercuboid of xi is: lj = χ(wvi,j + c1(bi,j − xi,j) + c2(gi,j − xi,j)). (3) A particle xi can therefore effectively move to any point within this hypercuboid (determined by the draws of r1 and r2), but not outside of it. Note – depending on the values of χ, c1 and c2, it is possible for one or more of vi, pi and gi to lie outside this bounded region, and the higher the inertia, the more exploratory the search. As regions of the hypercuboid may lie outside of X, a PSO implementation must have a mechanism to deal with potential movements outside the legal bounds. We use rejection sampling here – i.e. r1 and r2 are resampled until a legal xi results. Here we exploit the swarm paradigm for multi-modal op- timisation. However, instead of employing a single swarm optimiser and using neighbourhood topology to maintain modes (e.g. [5]), we use multiple swarms – each of which is concerned with optimising a particular mode estimate that has been identified in the search landscape. The approach taken differs from other multi-swarm work for multi-modal problems (e.g. [14]), in that the sub-swarms do not take a “devour and move on approach”, but instead concern themselves solely with the improvement of their local peak estimate, from the time of that particular swarm’s initialisation, until the algorithm termination (or its merging with another swarm). Unlike other sub-swarm work which use distributed swarms (e.g. [15]) the number of swarms is dynamic, and expends far fewer evaluations on niche detection. The basic idea is that NMMSO manages a number of swarms which have strong local search, and which ‘fine- tune’ their local mode estimates each generation. Additionally, on each generation swarms which have improved their mode estimate are paired with their closest adjacent swarm to see if they should merge (thus preventing duplication of labour). New regions in which to search for modes are identified by splitting away particles from existing swarms, and via random search and crossover. A high level description of the algorithm can be found in Figure 1. The algorithm takes in four overarching parameters: max evals, tol, n and max inc, alongside the standard PSO parameters which are used by all the sub-swarms. The param- eter max evals sets the total number of permissible function evaluations. tol is a small tolerance value, which specifies the Euclidean distance in design space where two peak estimates (and associated swarms) are automatically merged (we use 10−6 in our experiments here). n is the maximum number of particles in any swarm, and max inc is the maximum number of swarms to increment per algorithm iteration. The algorithm starts by generating and evaluating a single solution at random within X , making the initial swarm (Figure 1, line 1). The algorithm then continues in an optimisation loop, until the allocated function evaluations are exhausted. Line 4 checks if any swarms are flagged. This will be the case if their gbest (mode estimate) has changed in the last algorithm iteration, or if the swarm has just resulted from the merging of two previous swarms. Those swarms that have been marked are compared to their nearest neighbour (based on the Euclidean distance between their respective gbests locations in design space). If the nearest neighbour is within tol distance, then the swarms are automatically merged, if not, then the mid-point in design space between the gbest locations is evaluated. If this location is worse than both of the swarm gbests, the pair are maintained separately – if not they are merged. The variable m tracks how many mid-points have been evaluated each time the routine is called. Sampling along a line for peak detection is not new (being introduced in [16]) – however typically multiple points are sampled each time, and the detection procedure is regularly undertaken (e.g. [16], [15]), consuming the majority of function evaluations during an optimisation. In contrast, NMMSO uses only a small fraction of its function evaluations on line sampling. When merging swarms, if the total number of particles in both swarms is less than, or equal to, the swarm limit n, then the resultant swarm simply contains all the elements of both swarms. If however the total number of elements in both exceeds n, then the fittest n particles across both swarms are used to create the merged swarm, with the remaining particles being discarded. Require: max ￼ evals, tol, n, max ￼ inc, c1, c2, χ, ω 1: S := initialise_swarm(1) 2: evals:=1 3: while evals &lt; max evals do 4: while flagged_swarms(S) = true do 5: {S, m} := attempt_merge(S, n, tol) 6: evals := evals + m 7: end while 8: S := increment(S, n, max inc, c1, c2, χ, ω) 9: evals := evals + min(|S|, max inc) 10: {S, k} := attempt_separation(S, tol) 11: evals := evals + k 12: S := add_new_swarm(S) 13: evals := evals + 1 14: end while 15: {X ∗ , Y ∗ } := extract_gbest(S ) 16: return X∗,Y∗ Fig. 1. High-level pseudocode of NMMSO. The evaluation of a mid-point between two mode estimate locations is a coarse way of mode region identification. There are a number of landscape conditions where it will identify two solutions as lying on different peaks when they are in fact on the same mode. This may occur if there is a ridge curving away and then back joining the two points, or when a point ‘lower down’ on the same peak region is closer to a point on another mode, than a point lying ‘further up’ on the same mode. This means the point lower down will be paired for comparison with the point on the other mode, resulting in both the mode estimates lying on different parts of the same peak region being maintained as gbests for distinct swarms. However, as the swarms optimise and improve their gbests (move up the peaks), mode estimates will move to positions where their relationship is correctly identified (resulting in merging). After merging has been attempted, each swarm is incre- mented (line 8). If a swarm has fewer than n particles, then a single new particle is added to the swarm and evaluated. This new entrant is sampled uniformly in a hypersphere centred on the swarm’s current peak estimate (its gbest), with the radius corresponding to half the distance to the nearest neighbouring swarm peak estimate (subject to the sample lying in X ). The velocity is sampled in a similar fashion (but centred on 0, without having to lie in X ). Note that although the particles are initialised within this local region, their subsequent movement is not restricted (beyond staying in X). If a swarm has reached its full quota of n particles, then on incrementing the swarm one of its particles is selected at random, and updated according to (1) and (2). The number of swarms incremented each generation is limited to max inc – if the number of swarms maintained exceeds this value, then 50% of the time the max inc swarms with the best gbests are incremented, the other 50% of the time max inc swarms are selected at random for incrementing. This limit is to prevent ￼￼￼￼￼￼￼￼￼￼ TABLE I PARAMETERS USED FOR PERFORMANCE MEASUREMENT, AND MAXIMUM NUMBER OF FUNCTION EVALUATIONS PER OPTIMISER RUN. V. EMPIRICAL RESULTS The NMMSO algorithm is compared to the results of a wide range of multi-modal optimisation algorithms [17], [7], [18], [8], [19], [20], [21], [6], [9], [22], which were applied to the 20 benchmark problems of the CEC 2013 “Competition on Niching Methods for Mutlimodal Optimization” [23], [1]. We follow the algorithm assessment protocol used in the CEC 2013 competition; our results can therefore be directly compared to the combined competition results. We also compare to the Multinational Evolutionary Algo- rithm (MEA) [16] and Multi-Sub-Swarm Particle Swarm Op- timisation Algorithm (MSSPSO) [15] – as these both embed ‘hill-valley’ approaches to niche maintenance. Parameters for these comparison algorithms are fixed as in their respective original publications. In [16] the population size is varied across test problems from 750-1500, we use 1000 for all test problems here. Likewise in [15] population size varied from 1-40 and number of sub-swarms from 5-25. In keeping with the original competition, algorithm parameters are fixed for all problems – a population size of 20 and 25 sub-swarms is used here for MSSPSO.2 The 20 benchmark problems are of varying dimensionality and number of optima, and are derived from 12 base test prob- lems. The Equal Maxima (F2), Himmelblau (F4), Vincent (F7, F9) and Modified Rastrigin (F10) problems only have global peaks. The Five-Uneven-Peak Trap (F1), Uneven Decreasing Maxima (F3), Shubert (F6, F8), and Composite Functions 1 (F11), 2 (F12), 3 (F13, F14, F16, F18) and 4 (F15, F16, F19, F20) all have local maxima as well as global maxima (with the Shubert and Composite Functions having many more local maxima than global maxima). Due to space constraints formal definitions are not provided here – however a technical report detailing them can be found online [23]. Additionally, NMMSO is compared to the problem level results which are published in [9] and [8] for the niching vari- able mesh optimisation (N-VMO) and the dynamic archiving niching differential evolution (dADE) algorithms. Problem assessment criteria are detailed in Table I. The parameter r gives the maximum distance (in design space) a solution may be from a peak be categorised to have found it – subject to a further accuracy level, ε, which gives the maximum distance from the global maximum in objective space. For all problems five different accuracy levels are assessed, ε = {10−1 , 10−2 , 10−3 , 10−4 , 10−5 }. All algorithms are run 50 times on each problem. Two cri- teria are used for assessment. The success rate (SR) measures the proportion of successful runs (those which find all global optima given the prescribed ε and r). A value of 1.0 indicates that all 50 runs found all global peaks, whereas a value of 0.5 indicates that half of the runs found all global peaks. The peak ratio (PR) measure gives the average proportion of global 2The value of the penalty applied to swarm members who stray onto other peaks is not detailed in [15]. Here we set it such that straying particles cannot replace their pbest. ￼￼Function ￼F1 ￼￼F2 F3 ￼￼F4 ￼F5 ￼r Peak height Max evals ￼￼￼0.01 200 50k ￼￼￼￼0.01 1 50k ￼￼￼￼0.01 1 50k ￼￼0.01 200 50k ￼￼￼￼￼0.5 1.03163 50k ￼￼￼Function ￼F6 ￼￼F7 F8 ￼￼F9 ￼F10 ￼r Peak height Max evals ￼￼￼0.5 186.731 200k ￼￼￼￼0.2 1 200k ￼￼￼￼0.5 2709.0935 400k ￼￼0.2 1 400k ￼￼￼￼￼0.01 -2 200k ￼￼￼Function ￼F11 ￼￼F12 F13 ￼￼F14 ￼F15 ￼r Peak height Max evals ￼￼￼0.01 0 200k ￼￼￼￼0.01 0 200k ￼￼￼￼0.01 0 200k ￼￼0.01 0 400k ￼￼￼￼￼0.01 0 400k ￼￼￼Function ￼F16 ￼￼F17 F18 ￼￼F19 ￼F20 ￼r Peak height Max evals ￼￼￼0.01 0 400k ￼￼￼￼0.01 0 400k ￼￼￼￼0.01 0 400k ￼￼0.01 0 400k ￼￼￼￼￼0.01 0 400k ￼￼￼￼￼￼￼￼￼￼￼￼￼the algorithm exhausting too many function evaluations on poor performing local modes. It biases the search toward the best max inc found so far, but still searches in the wider population of swarms, as these swarms may find substantially better solutions at a later point in time and may also provide useful particles when merging. On line 10 a single swarm is selected at random from those which are at capacity (i.e. have n elements). This is checked to see if it should have a particle removed to seed a new swarm. First a particle, xi, is selected at random from the swarm. If xi is more than tol distance from the swarm’s gbest, then the mid-point between it and the swam gbest is evaluated.1 If the performance of this mid-point is worse than f(xi), then xi is removed from its parent swarm, and used to seed a new swarm, taking its velocity with it. It is replaced in its previous swarm by the evaluated mid-point, whose velocity is initialised at random as if it was a new swarm entrant. If however the mid-point is not worse than xi, then xi remains in its original swarm, and instead the swarm’s gbest is compared to the mid-point evaluation: if it is worse, then the gbest is replaced. Very early in an optimiser run, as none of the maintained swarms has reached capacity, no mid-points are evaluated due to the attempt_separation routine (i.e. k = 0). As the optimisation progresses, typically k = 1 at each iteration, although as swarms converge on their gbest locations the randomly selected xi may be within the tol distance, and so k = 0 may still occur later in an optimisation run. Finally, each algorithm iteration ends with a new swarm being generated. 50% of the time this is seeded with a uniformly sampled location in X , otherwise it is seeded by a single offspring solution, generated using uniform crossover of two randomly chosen swarm gbests (the probability of inheriting a particular design parameter from either parent being 0.5). The generation of random swarms means the algorithm is constantly looking for new peaks in addition to those currently being optimised, and the uniform crossover means any peak symmetry in the landscape is exploited. 1If it is less than tol distance away, even if it is split off, it will be merged back into the swarm on line 5 of the next iteration, so it is computationally efficient to check this beforehand. ￼ ￼TABLE II CONVERGENCE RATES OF NMMSO, N-VMO AND DADE/NRAND/1/BIN. BEST VALUES FOR EACH PROBLEM ARE UNDERLINED. 10−1 1.000 10−2 1.000 10−3 1.000 10−4 1.000 10−5 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 F6 F7 F8 F9 F10 Algorithm dADE/nrand/1 NMMSO Algorithm dADE/nrand/1 NMMSO Algorithm dADE/nrand/1 N-VMO NMMSO Algorithm dADE/nrand/1 N-VMO NMMSO Function Mean St. D. Mean St. D. F1 F2 5922 221 1673 38 578 167 135 62 F11 F12 145456 114735 58240 21749 5836 49537 ε = 10−1 F3 F4 F5 203 3107 367 14 845 121 46 1191 114 46 947 53 F13 F14 F15 182185 219869 61965 47527 154230 16890 53038 391400 400000 F6 27459 6904 68441 42978 F16 292773 133136 F7 2911 618 27349 14198 F17 200503 127782 F8 367282 42450 391589 21682 F18 392376 F9 F10 396811 3392 22547 653 399982 1422 125 444 F19 F20 Function Mean St. D. Mean 340214 400000 St.D. 2872 41905 40697 36202 0 0 0 0 0 0 Function Mean St. D. Mean St.D. Mean St. D. Function Mean St.D. Mean St.D. Mean St. D. F1 F2 20202 1801 2788 586 12795 31835 236 2847 1089 487 179 124 F11 F12 200000 200000 ε = 10−4 F3 F4 F5 1290 12703 3567 565 1668 652 380 25769 13012 128 265 278 0 0 0 0 24688 342 1910 617 131 913 149 F13 F14 F15 200000 400000 400000 88759 40262 F16 400000 31350 13175 F17 400000 392525 21587 F18 400000 400000 2686 0 388 F19 F20 400000 400000 0 0 0 0 0 0 0 0 0 0 200000 200000 200000 400000 400000 400000 400000 400000 400000 400000 0 0 0 0 0 0 0 0 0 0 9057 68049 74120 400000 400000 400000 400000 400000 400000 400000 3261 38807 43854 0 0 0 0 0 0 0 TABLE III SUCCESS RATES OF NMMSO, N-VMO AND DADE/NRAND/1/BIN. BEST VALUES FOR EACH PROBLEM AND ACCURACY LEVEL ARE UNDERLINED. F1 F2 F3 F4 F5 ε NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE ε NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE 10−1 0.960 1.000 1.000 1.000 1.000 1.000 0.260 0.000 0.020 0.020 1.000 1.000 1.000 1.000 0.500 10−2 0.960 1.000 1.000 1.000 1.000 0.240 0.220 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.380 10−3 0.960 0.360 1.000 1.000 0.140 0.020 0.180 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.280 10−4 0.940 0.000 0.780 1.000 0.000 0.000 0.180 0.000 0.000 0.000 0.000 1.000 1.000 1.000 0.140 10−5 0.000 0.000 0.000 1.000 0.000 0.000 0.180 0.000 0.000 0.000 0.000 1.000 1.000 0.660 0.020 F11 F12 F13 F14 F15 ε NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE 10−1 1.000 1.000 0.640 0.980 0.220 0.980 0.960 1.000 0.140 0.080 1.000 0.700 0.000 1.000 1.000 10−2 1.000 0.000 0.000 0.980 0.000 0.440 0.960 0.000 0.000 0.060 0.000 0.000 0.000 0.020 0.000 10−3 1.000 0.000 0.000 0.980 0.000 0.000 0.940 0.000 0.000 0.020 0.000 0.000 0.000 0.000 0.000 10−4 1.000 0.000 0.000 0.980 0.000 0.000 0.940 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 10−5 1.000 0.000 0.000 0.980 0.000 0.000 0.940 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 F16 F17 F18 F19 F20 ε NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE NMMSO N-VMO dADE 10−1 0.000 1.000 0.540 0.000 10−2 0.000 0.020 0.000 0.000 10−3 0.000 0.000 0.000 0.000 10−4 0.000 0.000 0.000 0.000 10−5 0.000 0.000 0.000 0.000 peaks found across runs, i.e. for q runs: q o PR= i=1 i tq 1.000 0.760 0.000 0.960 0.080 0.000 0.220 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 where oi denotes the number of global optima discovered by the ith run, and t is the total number of global peaks. We use the code made available by the CEC 2013 competition organisers for representing the test problems, and for assessing algorithm performance.3 We set the automatic merging tolerance tol = 10−6, the maximum swarm size n = 10D (where D is the number of design parameters) and the maximum number of swarms to increment max inc = 100. We use standard PSO parameters 3 Obtainable from http://goanna.cs.rmit.edu.au/∼xiaodong/cec13- niching/ competition/. (4) c1 , c2 = 2.0, χ = 1.0 but with a low inertia to promote local convergence (ω = 0.1). Table II shows the convergence rates of NMMSO and dADE for ε = 10−1, and these plus N-VMO for ε = 10−4 (convergence results for ε = 10−1 are not reported in [9]). At the ε = 10−1 level dADE performs the best, having the fastest mean convergence 10 times to NMMSO’s nine. At the ε = 10−4 level however NMMSO performs much better than both the other algorithms, having faster convergence at this level on 12 of the problems. On the other eight problems none of the algorithms converged to all the global optima on any runatε=10−4. Tables III and IV give the success rate and peak ratio results across the 20 test problems for all five levels of accuracy for each of the algorithms. NMMSO has the best/equal best SR F6 F7 150328 200000 F8 393667 17665 400000 F9 F10 400000 12904 0 2169 35209 0 200000 200000 400000 181772 400000 400000 33324 400000 400000 95904 0 400000 TABLE IV MEAN PEAK RATIOS OF NMMSO, N-VMO AND DADE/NRAND/1/BIN. BEST VALUES FOR EACH PROBLEM AND ACCURACY LEVEL ARE UNDERLINED. ￼￼￼F1 F2 F3 F4 F5 ￼￼ε NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼N-VMO ￼￼dADE 10−1 1.000 10−2 1.000 10−3 1.000 10−4 1.000 10−5 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼F6 F7 F8 F9 F10 ￼￼ε NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼N-VMO ￼￼dADE 10−1 0.998 10−2 0.998 10−3 0.998 10−4 0.997 10−5 0.000 ￼￼￼￼￼￼￼￼1.000 1.000 0.940 0.670 0.000 ￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 0.984 1.000 0.000 1.000 ￼￼￼￼￼￼￼￼￼￼1.000 1.000 0.945 0.901 0.806 ￼￼￼￼￼￼￼￼￼￼1.000 0.984 0.962 0.984 0.892 0.983 0.823 0.981 0.732 0.980 ￼￼￼￼￼￼￼￼0.412 0.294 0.270 0.198 0.027 ￼￼￼￼￼￼￼￼0.837 0.930 0.595 0.922 0.545 0.920 0.431 0.917 0.356 0.913 ￼￼1.000 0.683 0.399 0.275 0.192 ￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ￼￼￼￼￼￼￼￼￼￼￼1.000 1.000 1.000 1.000 0.968 ￼￼￼￼￼￼￼￼￼￼0.985 0.978 0.981 0.967 0.947 ￼￼￼￼￼￼￼￼￼￼￼F11 F12 F13 F14 F15 ￼￼ε NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼N-VMO ￼￼dADE 10−1 1.000 10−2 1.000 10−3 1.000 10−4 1.000 10−5 1.000 ￼￼￼￼￼￼￼￼￼￼￼￼1.000 0.667 0.667 0.667 0.667 ￼￼￼￼￼￼￼￼￼0.893 0.998 0.667 0.998 0.667 0.998 0.667 0.998 0.667 0.998 ￼￼￼￼￼￼￼0.848 0.745 0.725 0.713 0.565 ￼￼￼￼￼￼￼￼0.998 0.993 0.887 0.993 0.745 0.990 0.740 0.990 0.728 0.990 ￼￼￼￼￼￼￼1.000 0.667 0.667 0.667 0.663 ￼￼￼￼￼￼￼￼￼0.743 0.770 0.667 0.740 0.667 0.713 0.667 0.710 0.667 0.703 ￼￼￼￼￼￼1.000 0.667 0.667 0.667 0.637 ￼￼￼￼￼￼￼￼￼0.923 0.673 0.667 0.673 0.655 0.673 0.655 0.670 0.655 0.668 ￼￼￼￼1.000 0.713 0.668 0.623 0.390 ￼￼￼￼￼￼￼￼1.000 0.620 0.615 0.627 0.620 ￼￼￼￼￼￼￼￼￼￼￼￼F16 F17 F18 F19 F20 ￼￼ε NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼￼N-VMO dADE NMMSO ￼N-VMO ￼￼dADE 10−1 0.663 10−2 0.660 10−3 0.660 10−4 0.660 10−5 0.660 ￼￼￼￼￼￼￼1.000 0.703 0.653 0.653 0.633 ￼￼￼￼￼￼￼￼￼￼0.873 0.553 0.667 0.548 0.667 0.543 0.667 0.538 0.667 0.538 ￼￼￼￼￼￼￼￼￼1.000 0.475 0.440 0.413 0.320 ￼￼￼￼￼￼￼￼￼0.938 0.633 0.472 0.633 0.417 0.633 0.403 0.633 0.410 0.633 ￼￼￼￼￼0.987 0.483 0.470 0.470 0.360 ￼￼￼￼￼￼￼￼￼0.683 0.477 0.660 0.470 0.630 0.463 0.633 0.447 0.627 0.443 ￼￼￼￼￼￼￼￼￼0.340 0.133 0.133 0.130 0.103 ￼￼￼￼￼￼￼￼0.420 0.183 0.143 0.180 0.063 0.178 0.018 0.178 0.000 0.178 ￼￼￼￼￼￼0.000 0.000 0.000 0.000 0.000 ￼￼￼￼￼￼0.030 0.000 0.002 0.005 0.000 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼value on 57% of the problem/accuracy level combinations. N- VMO achieves this for 44% and dADE 36%.4 For 26% of the problem/accuracy level combinations all of the optimisers have an SR of zero. On the mean PR measure, NMMSO has the best/equal best value on 79% of the problem/accuracy level combinations. N-VMO achieves this for 43% and dADE 41%. No algorithm found any solutions at the ε = 10−5 level on F6. NMMSO tends to have relatively better performance at the higher accuracy levels. This would indicate that once NMMSO has identified a peak, the low inertia swarms are more effective at exploiting it to a high level of accuracy in a rapid fashion compared to the other algorithms. Table V presents the overall performance assessment of NMMSO. The mean PR for the five accuracy levels on each of the 20 test functions are combining into a single average, and compared to the results of the 15 state-of-the-art entrants to the CEC 2013 competition (results from [1]), along with MEA and MSSPSO. NMMSO can be seen to be extremely competitive with the current state-of-the-art, having the highest mean and median PR yet observed. Figure 2 shows the number of swarms maintained by each run of NMMSO for each of the problems. As can be seen, when there are only global modes, the number of swarms maintained converges to the number of global modes (F2, F4, F7, F9 and F10). For problems with very many local optima the number of swarms can be seen to constantly rise, however, due to the search bias toward the better performing mode estimates, this does not prevent good convergence results on these problems. Interestingly, on the composite functions with 10-20D NMMSO can be seen to consistently hone just a few peaks until at some point the number of swarms rises quickly. 4There appear to be some data entry issues in the tabulated results for the some of peak ratios in [8], as the values reported increase from ε = 10−3 to 10−4 for F15 and F18, and from ε = 10−2 to 10−4 for F20. TABLE V AVERAGE RESULTS ACROSS ALL TEST PROBLEMS AND ACCURACY LEVELS OF NMMSO, MEA AND MSSPSO, ALONG WITH RESULTS OF MULTI-MODAL ALGORITHMS COMPARED IN THE CEC 2013 COMPETITION (DETAILED IN [1]) ON THE PEAK RATIO. ￼￼Algorithm ￼Median ￼￼Mean ￼St. D. ￼NMMSO MEA [16] MSSPSO [15] ￼￼￼0.9933 0.2167 0.0000 ￼￼￼￼￼0.8271 0.3676 0.2179 ￼￼￼￼￼￼0.2535 0.3878 0.3901 ￼￼￼A-NSGA-II [17] CMA-ES [7] CrowdingDE [18] dADE/nrand/1 [8] dADE/nrand/2 [8] DECG [19] DELG [19] DELS-aj [19] DE/nrand/1 [20] DE/nrand/2 [20] IPOP-CMA-ES [21] NEA1 [6] NEA2 [6] N-VMO [9] PNA-NSGA-II [22] ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0.0740 0.7750 0.6667 0.7488 0.7150 0.6567 0.6667 0.6667 0.6386 0.6667 0.2600 0.6496 0.8513 0.7140 0.6660 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0.3275 0.7137 0.5731 0.7383 0.6931 0.5516 0.5706 0.5760 0.5809 0.6082 0.3625 0.6117 0.7940 0.6983 0.6141 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0.4044 0.2807 0.3612 0.3010 0.3174 0.3992 0.3925 0.3857 0.3338 0.3130 0.3117 0.3280 0.2332 0.3307 0.3421 ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼This is probably due to tendency of NMMSO to merge local modes that live on larger landscape features. Figure 3 shows the growth of a swarm population over time as visualised in X for some of the 2D test problems. Where the global/local peaks/basins are deformations from the plane the number of modes identified quickly increases (see e.g. F6, and regions in F12 and F13), as the mid-points always tend to be lower between any pairing of global/local mode estimate. Where however local modes lie on a larger landscape feature (e.g. the peak mid way down on the right of F11 and F12), these swarms tend not to be sustained, as pairing with a swarm converging on a mode ‘further’ up the larger landscape feature ￼10 10 10 10 10 F1 F2 F3 F4 F5 # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms # Swarms 55555 00000 0 300 200 100 0 1000 Iteration 20 F11 15 10 5 400 400 300 200 400 200 F6 F7 40 600 F8 200 F9 150 100 100 Iteration 200 0 50 50 Iteration 100 0 20 40 60 80 Iteration 0 500 1000 0 Iteration 50 100 150 Iteration F10 100 200 Iteration 200 200 2000 2000 0 1000 2000 3000 Iteration F13 50 0 2000 Iteration 4000 0 30 20 10 0 1000 Iteration 150 100 50 400 10 5 200 00000 F12 F14 600 F15 400 200 60 40 20 200 100 00000 0 1000 2000 0 1000 2000 0 2000 4000 0 2000 4000 0 2000 4000 Iteration Iteration Iteration Iteration F16 F17 F18 F19 40 30 20 10 Iteration F20 100 00000 0 2000 4000 6000 0 2000 4000 0 2 4 0 2 4 0 2 4 6 Iteration Iteration Iteration x 104 Iteration x 104 Iteration x 104 Fig. 2. Number of swarms maintained by each run on each problem, recorded at each iteration until all global optima have converged to within 10−5, or the function evaluations allowed are exhausted. Swarms recorded at line 11 of Figure 1. Mean of runs plotted in red (when a run has terminated, the size of its final population is used in the calculation of the mean swarm size until all runs complete). will tend to result in a mid-point that is higher than the lower swarm gbest – causing the paired swarms to be merged. VI. DISCUSSION We have introduced a new multi-modal optimiser which utilises a number of self-contained but communicating sub- swarms to search for modes in the fitness landscape. The approach builds on a number of properties which have been identified in effective multi-modal optimisers, and continu- ously hones mode estimates by exploiting individual swarms in parallel, rather than searching in local regions and moving on like other multi-swarm approaches to multi-modal optimi- sation. Particles may additionally split off to form new swarms, or migrate between swarms by splitting and subsequently merging. Results on the CEC 2013 niching test problems show that the proposed algorithm is extremely competitive with the current state-of-the-art, and gives robust performance, even though seeded by a single random solution in X. Nevertheless, the number of parameters is still larger than would be generally desirable: future work will be focused on reducing these (via self-adaptation) as well as applying NMMSO to multi-modal engineering design problems. Note however that the optimiser does not require a niching radius or number of niches to be set – as it dynamically fits both of these locally, based upon the distance between the current peak estimate locations being maintained. MATLAB code for the NMMSO, MEA and MSSPSO algorithms is available at https://github.com/fieldsend. ACKNOWLEDGEMENT The author would like to thank Prof. Xiaodong Li, Prof. Andries Engelbrecht, and Dr Michael Epitropakis, the organ- isers of the CEC 2013 “Competition on Niching Methods for Multimodal Optimization”, for making the competition test problem code and evaluation functions readily available online for the community. REFERENCES [1] X. Li, A. Engelbrecht, and M. Epitropakis, “Results of the 2013 IEEE CEC Competition on Niching Methods for Multimodal Optimization.” Presented at 2013 IEEE Congress on Evolutionary Computation Com- petition on: Niching Methods for Multimodal Optimization, 2013. [2] J. Holland, Adaptation in Natural and Artificial Systems. University of Michigan Press, 1975. [3] D. E. Goldberg and J. Richardson, “Genetic algorithms with sharing for multimodal function optimisation,” in Proceedings of the Second International Conference on Genetic Algorithms and their Application, 1987, pp. 41–49. [4] B. Sareni and L. Kra ̈henbu ̈hl, “Fitness Sharing and Niching Methods Revisited,” IEEE Transactions on Evolutionary Computation, vol. 2, no. 3, pp. 97–106, 1998. [5] X. Li and K. Deb, “Comparing lbest PSO niching algorithms using different position update rules,” in IEEE Congress on Evolutionary Computation, 2010, pp. 1564–1571. [6] M. Preuss, “Niching the CMA-ES via Nearest-better Clustering,” in Proceedings of the 12th Annual Conference Companion on Genetic and Evolutionary Computation, ser. GECCO ’10, 2010, pp. 1711–1718. ￼Function evaluations 25 250 2500 25000 250000 10 10 10 10 10 55555 00000 −5 −5 −5 −5 −5 −10 −10 −10 −10 −10 −10 −5 0 5 10 −10 −5 0 5 10 −10 −5 0 5 10 −10 −5 0 5 10 −10 −5 0 5 10 55555 F110 0 0 0 0 −5 −5 −5 −5 −5 −5 0 5−5 0 5−5 0 5−5 0 5−5 0 5 55555 F120 0 0 0 0 −5 −5 −5 −5 −5 −5 0 5−5 0 5−5 0 5−5 0 5−5 0 5 55555 F130 0 0 0 0 −5 −5 −5 −5 −5 −5 0 5−5 0 5−5 0 5−5 0 5−5 0 5 Fig. 3. Swarm gbests (marked with a ‘⊗’), and swarm particles (marked with a ‘·’) of NMMSO at different numbers of function evaluations for five of the 2D test problems. Function response coloured from high (dark red) to low (dark blue). F6 [7] N. Hansen and A. Ostermeier, “Completely Derandomized Self- Adaptation in Evolution Strategies,” Evolutionary Computation, vol. 9, no. 2, pp. 159–195, 2001. [8] M. G. Epitropakis, X. Li, and E. K. Burke, “A dynamic archive niching differential evolution algorithm for multimodal optimization,” in IEEE Congress on Evolutionary Computation, 2013, pp. 79–86. [9] D. Molina, A. Puris, R. Bello, and F. Herrera, “Variable mesh optimiza- tion for the 2013 CEC Special Session Niching Methods for Multimodal Optimization,” in IEEE Congress on Evolutionary Computation, 2013, pp. 87–94. [10] J. E. Fieldsend, “Multi-Modal Optimisation using a Localised Surrogates Assisted Evolutionary Algorithm,” in UK Workshop on Computational Intelligence (UKCI 2013), 2013, pp. 88–95. [11] ——, “Using an adaptive collection of local evolutionary algorithms for multi-modal problems,” Soft Computing, in press. [12] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in IEEE International Conference on Neural Networks. Perth, Australia: IEEE Service Center, 1995, pp. 1942–1948. [13] J. Fieldsend, “Multi-objective particle swarm optimisation methods,” Department of Computer Science, University of Exeter, Tech. Rep. 419, March 2004. [14] S. Chen, “Locust Swarms - A new multi-optima search technique,” in IEEE Congress on Evolutionary Computation, 2009, pp. 1745–1752. [15] J. Zhang, D.-S. Huang, and K.-H. Liu, “Multi-Sub-Swarm Optimization Algorithm for Multimodal Function Optimization,” in IEEE Congress on Evolutionary Computation, 2007, pp. 3215–3220. [16] R. K. Ursem, “Multinational evolutionary algorithms,” in Proceedings of the Congress on Evolutionary Computation, 1999, pp. 1633–1640. [17] K. Deb and A. Saha, “Multimodal Optimization Using a Bi-objective Evolutionary Algorithm,” Evolutionary Computation, vol. 20, no. 1, pp. 27–62, 2012. [18] R. Thomsen, “Multimodal optimization using crowding-based differen- tial evolution,” in IEEE Congress on Evolutionary Computation, 2004, pp. 1382–1389. [19] J. Ronkkonen, “Continuous Multimodal Global Optimization with Dif- ferential Evolution-Based Methods,” Ph.D. dissertation, Lappeenranta University of Technology, Finland, 2009. [20] M. G. Epitropakis, V. P. Plagianakos, and M. N. Vrahatis, “Finding mul- tiple global optima exploiting differential evolution’s niching capability,” in IEEE Symposium on Differential Evolution (SDE), 2011, pp. 1–8. [21] A. Auger and N. Hansen, “A restart CMA evolution strategy with increasing population size,” in IEEE Congress on Evolutionary Com- putation, vol. 2, 2005, pp. 1769–1776. [22] S. Bandaru and K. Deb, “A parameterless-niching-assisted bi-objective approach to multimodal optimization,” in IEEE Congress on Evolution- ary Computation, 2013, pp. 95–102. [23] X. Li, A. Engelbrecht, and M. G. Epitropakis, “Benchmark Functions for CEC’2013 Special Session and Competition on Niching Methods for Multimodal Function Optimization,” Evolutionary Computation and Machine Learning Group, RMIT University, Tech. Rep., 2013.</Text>
        </Document>
        <Document ID="28">
            <Title>CEC Algorithms</Title>
        </Document>
        <Document ID="13">
            <Title>CEC</Title>
            <Text>The IEEE Congress of Evolutionary Computation (CEC) is one of the largest, most important and recognized conferences within Evolutionary Computation (EC). It is organised by the IEEE Computational Intelligence Society in cooperation with the Evolutionary Programming Society and covers most of the subtopics of the EC [^2].

In order to validate the potential of the NMMSO algorithm, it was submitted to the IEEE CEC 2015 held in Sendai, Japan. Here, Dr. Fieldsend was provided with some multimodal benchmark test functions with different dimension sizes and characteristics, for evaluating niching algorithms developed by Dr. Xiaodong Li, Dr. Andries Engelbrecht and Dr. Michael G. Epitropakis [@li_2013]. They state that even if several niching methods have been around for many years, further advances in this area have been hindered by several obstacles; most of the studies focus on very low dimensional multi-modal problems (2 or 3 dimensions) making this more complicated to assess theses methods’ scalability to high dimensions with better performance. The benchmark tool includes 20 test functions (in some cases the same function but with different dimension sizes), which includes 10 simple, well-known and widely used benchmark functions, based on recent studies, and more complex functions following the paradigm of composition functions. In the following section, they will be briefly explained:

	•	F1: Five-Uneven-Peak Trap (1D)
	•	F2: Equal Maxima (1D)
	•	F3: Uneven Decreasing Maxima (1D)
	•	F4: Himmelblau (2D)
	•	F5: Six-Hump Camel Back (2D)
	•	F6: Shubert (2D, 3D)
	•	F7: Vincent (2D, 3D)
	•	F8: Modified Rastrigin - All Global Optima (2D)
	•	F9: Composition Function 1 (2D)
	•	F10: Composition Function 2 (2D)
	•	F11: Composition Function 3 (2D, 3D, 5D, 10D)
	•	F12: Composition Function 4 (3D, 5D, 10D, 20D)

All of the test functions are formulated as maximisation problems. F1, F2 and F3 are simple 1D multimodal functions, while F4 and F5 are simple 2D functions and not scalable. F6 to F8 are scalable multimodal functions. The number of global optima for F6 and F7 are determined by the dimension. However, for F8, the number of global optima is independent of the dimension, therefore it can be controlled by the user. F9 to F12 are scalable multimodal functions constructed by several basic functions with different properties (Sphere function, Grienwank, Rastrigin, Weierstrass and the Expanded Griewank’s plus Rosenbrock’s function). F9 and F10 are separable, and non-symmetric while F11 and F12 are non-separable, non-symmetric complex multimodal functions. The number of global optima in all of the composition functions is independent of the number of dimensions, therefore, can be controlled by the user [@li_2013].

The properties of each function can be seen in the following table:

Function 					Variables Range 						#GO						#LO
--------					---------------							--------------------	-------------------
Five-Uneven-Peak Trap		$x \in [0, 30]$ 						2						3
Equal Maxima				$x \in [0, 1]$ 							5						0
Uneven Decreasing Maxima 	$x \in [0, 1]$ 							1						4
Himmelblau					$x,y \in [6,6]$ 						4						0
Six-Hump Camel Back			$x \in [1.9, 1.9]$ 						2						2
							$y \in [1.1, 1.1]$ 
Shubert						$x_i \in [10,10]^D,i=1,2,...,D$			$D * 3D$ 				many
Vincent						$x_i \in [0.25,10]^D, i = 1,2,...,D$ 	$6^D$ 					0
Modified Rastrigin			$x_i \in [0,1]D, i = 1,2,...,D$ 		$\Pi^Di=1^{ki}$			0
--------					---------------							--------------------	-------------------
Table: Properties of the test functions used. (#GO = Number of Global Optima, #LO = Number of Local Optima)

For simplifying purposes, the composition functions were not taken in consideration because of its complexity, for further and deeper understanding of all functions, please refer to the CEC’2013 Niching Benchmark Tech Report  [@li_2013].

Besides the 12 different functions, also, a $count\_goptima$ function was included in order to find the optimal value for each evaluated function in each iteration. Here, some optimal values are already given as well as the number of each value to find. Together with an accuracy rate, the evaluation starts in a cycle and stores the possible optimal values in order to be compared with the expected values using the accuracy rates of 0.1, 0.01, 0.001, 0.0001 and 0.00001 for each different iteration [@li_2013, p.7]. 

The structure of the $count\_goptima$ function can be seen as a pseudocode:

	input : 			
	Lsorted #a list of individuals (candidate solutions) 
			#sorted in decreasing fitness values;	    
	acc 	#accuracy level; 
	r 		#niche radius;		    
	ph 		#the fitness of global optima (or peak height) 
	output: 
	S  		#a list of best-fit individuals identified as solutions 
	
	begin 
		S = empty;		
		while not reaching the end of Lsorted do 
			Get best unprocessed p partOf Lsorted;
			found = FALSE;			
			if d(ph, f it(p)) &lt;= acc) then 
				for all s partOf S do					
					if d(s, p)  r then 
						found = TRUE; 
						break; 
					end 
				end 
				if not found then 
					let S = S + {p}; 
				end 
			end 
		end 
	end 

Once the optimal values are found and after comparing them with the same results of the Matlab implementation, it can be concluded that the new implementation works as expected and is ready for submission.

[^2]: https://en.wikipedia.org/wiki/IEEE_Congress_on_Evolutionary_Computation</Text>
        </Document>
        <Document ID="24">
            <Title>Acknowledgements</Title>
            <Text>We want to thank Dr. Jonathan Fieldsend for his continuous help via mail during this seminar. Also, the committee of the CEC was always available for questions and concerns during our work. Furthermore, a special thanks go to all employees of the chair for 'Information Systems and Statistics' including Dr. Mike Preuß, Jakob Bossek and Pascal Kerschke who were available for any questions regarding the implementation and this report at all times.
\newpage</Text>
        </Document>
        <Document ID="18">
            <Title>Discussion</Title>
            <Text>test</Text>
        </Document>
        <Document ID="29">
            <Title>Implementation and Pitfalls</Title>
            <Text>During the development, an issue raised with the CEC benchmark tool. In order to compare the R implementation of the NMMSO algorithm with the original one, it was mandatory to use this tool to test each of its functions with the new algorithm and compare results. After several complications with the original test suite (these complications will be addressed in the pitfalls’ section), it was decided to recode each of the functions as an independent R package to avoid any further complication and having an easier and more trustworthy comparison of the NMMSO algorithm in R.

The original tool included several files in order to build the main file calling each one and be able to run as needed, including a graphing function for presentation purposes. While re-writing the tool in R, it followed the same structure but just focusing in the main files like the functions and count_goptima files because most of the other files were used in the graphing function and in this case, the original file was not used but still was created in a way to present the output in a graphic way for this paper. In comparison with the NMMSO algorithm, this tool was faster to work with but still generated several issues during the process.

Finally, the whole benchmark tool uses a demo_suite file in order to run it and display the optimal values found using each of the twenty functions with dummy data. This file is considered as one of the most important files within the tool because it helped to understand how the original version used to work. Most of the tests were run with this function and comparing both versions several times until the output given was exactly the same and also focusing in the execution time so it could have a better performance and could be used as a new option in future cases for the CEC. The output of each function can be seen in the following R output:

**Results in R**	
	
	f_ 1  : f(1...1) =  120 
	f_ 2  : f(1...1) =  5.270904e-92 
	f_ 3  : f(1...1) =  0.02501472 
	f_ 4  : f(1...1) =  94 
	f_ 5  : f(1...1) =  -3.233333 
	f_ 6  : f(1...1) =  -3.180351 
	f_ 7  : f(1...1) =  0 
	f_ 8  : f(1...1) =  5.671692 
	f_ 9  : f(1...1) =  0 
	f_ 10  : f(1...1) =  -38 
	f_ 11  : f(1...1) =  -268.6638 
	f_ 12  : f(1...1) =  -758.9333 
	f_ 13  : f(1...1) =  -613.5412 
	f_ 14  : f(1...1) =  -1838.556 
	f_ 15  : f(1...1) =  -1049.86 
	f_ 16  : f(1...1) =  -2149.558 
	f_ 17  : f(1...1) =  -1238.211 
	f_ 18  : f(1...1) =  -1683.184 
	f_ 19  : f(1...1) =  -1342.819 
	f_ 20  : f(1...1) =  -1337.852 


When the decision of re-writing the whole benchmark tool in the R programming language was made, some issues came up regarding the way Matlab handles the matrices and vectors. After analysing the Matlab implementation, it was easier to solve the issues because it was done after the NMMSO implementation and some of these issues were repeated between both implementations. Even if the matrix and vectors caused some problems during the implementation, it was not necessary to use the add_row and add_col functions because it always uses a matrix of one row so it was decided to change the structure so the R implementation would only use vectors to facilitate and simplify its usage. 

After some testing, some issues raised regarding the performance of the tool. As explained in the section before, the tool contains 12 different functions in order to evaluate the algorithm’s data and together with the count_goptima function, get the optimal values for the different parameters given to the NMMSO algorithm. The main problem here was with the last 4 functions (composition functions), some data of an output file was mandatory read and given as a parameter for a further evaluation, making necessary the use of an additional R library in order to read this Matlab’s data files and use them as expected. This solution worked perfectly regarding the output values but increased the processing time because each file had to be read in each iteration so it was decided to create different global variables that read only once each Matlab’s data file and only be called during the iteration process. This worked as expected and reduced the time and processor consumption of the tool, displaying the same results as the Matlab implementation created by the CEC.</Text>
        </Document>
        <Document ID="14">
            <Title>Structure of the project</Title>
            <Text>In difference to the original implementation, it was chosen to split up all single functions of the nmmso algorithm into single files. These were bundled into the standard R package structure to give the possibility to make it available over CRAN[^3] in the future. To give the possibility to collaborate, the project was managed and versioned via Github. The package was tested with testthat[^4] and documented with roxygen2[^5]. To assure functioning the package was continuously tested with Travis CI.

After analysing the algorithm provided in Matlab by Dr. Fieldsend, it was decided to translate each of the functions into the R programming language first. At first instance, this task seemed to be simple because most of the functions were basically managing matrices and vectors, but later this became a problem that will be covered in the pitfalls’ section (4.2) of this paper.

Once all the NMMSO functions existed in R and having the input data, the testing phase started. It has been said, that one of the biggest problems when coding an already existing program into another programming language, is the different behaviors corresponding to each object (in case of an object-oriented language) or its main structure. The first runs came with several errors regarding the matrix generation and handling, slowing down the project in a near future. Using GitHub, it was easier to attack these problems in parallel, having one developer reviewing different functions and the other one, fixing other bugs and continue the testing phase. Also, this was achieved in an easier way, thanks to that each function was coded in an independent R file, making easier and faster the debugging and the fixing of each problem.

Having the algorithm running hand by hand with the CEC benchmark tool, also developed in R during the project, and in order to get the output needed to compare the new implementation with the old one written in Matlab, it was necessary to use the Palma Cluster. This gave a radical change to the project because it was never taken in consideration to use a bigger computational power. It was necessary to implement some batch jobs in order to run the algorithm within the cluster and then specify a way to print and store all the output data later to be analysed and compared. Even if this step was never considered part of the project, it came across in a helpful way because it helped to discover that the NMMSO.R implementation is working properly and in a way that it could be used for further research and investigations.

[^3]: Comprehensive R Archive Network. 
[^4]: https://cran.r-project.org/web/packages/testthat/index.html
[^5]: https://cran.r-project.org/web/packages/roxygen2/index.html
</Text>
        </Document>
        <Document ID="20">
            <Title>Introduction</Title>
            <Text>In the recent years, R has become the statistical programming language of choice for many scientists. The strength of R, being a domain specific language, has also become one of its weaknesses. Since new research findings in statistical computing are split up over several languages like R, Matlab or SciPy[^1], it often becomes difficult to compare new methods with established ones. Because it is also hard to interface those languages due to different architectures and data storage mechanisms there is often no other way than to reimplement new methods in a different programming language to create a common scope.

An example for a well perceived new finding in statistical computing is the NMMSO-Algorithm by Jonathan E. Fieldsend [@fieldsend_2014]. It won the niching competition in 2015 held by the CEC and is only written in Matlab. Since the chair 'Information Systems and Statistics' at the Westfälische Wilhelms-Universität Münster, Germany, is mainly concentrating its work on Statistical Computing in R, an implementation of this algorithm became interesting. 

As part of this Seminar Project in the context of the Seminar 'Statistical Computing in R', a reimplementation of the NMMSO algorithm in R (nmmso.R) will be presented. During this technical documentation, the general function of the algorithm and the used test cases by the CEC will be shown. Afterwards the structure and used techniques and libraries, as well as problems and pitfalls due to the different behaviors of R and Matlab, will be shown. The documentation will be closed by the benchmarking results and several test cases. 

It was the goal of this project to keep up high comparability with the original code, to ensure the correct functionality and easily implement changes to the original codebase in this program. To reach this, unit tests were used where possible and continuous comparison between interim results of the original implementation and nmmso.R where used to ensure functioning. Additionally a benchmarking suite, which builds on the CEC Benchmarking Suite for Niching Algorithm [@epitropakis_2013] was implemented to evaluate and test the functioning of nmmso.R with the same characteristics as in the original implementation.

[^1]: SciPy is a common library for the Python Programming language which brings Statistical Computing capabilities to the language.
\newpage</Text>
        </Document>
        <Document ID="9">
            <Title>The Implementation</Title>
        </Document>
    </Documents>
</SearchIndexes>
{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
{\fonttbl\f0\fnil\fcharset0 OpenSans;\f1\fmodern\fcharset0 Courier;\f2\fnil\fcharset0 Consolas;
\f3\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red196\green219\blue241;\red234\green234\blue234;\red107\green0\blue1;
\red43\green39\blue19;\red0\green0\blue120;\red19\green8\blue105;\red15\green112\blue1;\red83\green83\blue83;
}
\deftab720
\pard\pardeftab720\sl280

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
\
To compare nmmso.R with the original NMMSO the CEC test cases were used to run the same benchmarks as in the original submission [@fieldsend_2014]. There 4 different Ratios were used to measure the performance of certain algorithms. Three of those measures (Peak Ratio, Success Ratio and Convergence Speed) have been introduced in \kerning1\expnd0\expndtw0 [@epitropakis_2013, pp. 6-7] to create a common point of comparison. The fourth ratio is special for the nmmso algorithm since it tracks the number of swarms over the iterations of the algorithm. \expnd0\expndtw0\kerning0
Nmmso.R uses the same measures to reach the highest comparability possible.\
\
The first measure used is the Success Ratio (SR). The Success Ratio is defined as the percentage of successful runs (runs that found all global optima) over all runs [@li_2013, p. 7]. As for the other ratios this measure was taken over several independent runs and collectively evaluated. The taken measures for the Success Ratio can be found in Table 2. \
$$\cb2 \\frac\cb1 \{successful\\ runs\}\cb2 \{\cb1 NR\} = SR $$ \
Here $NR$ denotes the Number of runs done to reach this measure.\
\\newline
\f1\fs24 \
```\{r, SR, echo=FALSE\}\
\pard\pardeftab720\sl308
\cf0 rm(list = ls())\
# remove scientific notation\
library(plyr)\
\pard\pardeftab720\sl300

\f2\fs26 \cf0 \cb3 options(scipen=\cf4 999\cf0 )\cf5 \
\pard\pardeftab720\sl280

\f1\fs24 \cf0 \cb1 # calculate sr\
source("../R/cec_2015_problem_data.R")\
# calculate sr\
library(pander)\
\pard\pardeftab720\sl300

\f2\fs26 \cf0 \cb3 panderOptions(\cf4 'table.alignment.default'\cf0 , \cf6 function\cf0 (df) ifelse(sapply(df, is.numeric), \cf4 'right'\cf0 , \cf4 'left'\cf0 ))\
panderOptions(\cf4 'digits'\cf0 , \cf4 2\cf0 )\
panderOptions(\cf4 'round'\cf0 , \cf4 2\cf0 )\
panderOptions(\cf4 'keep.trailing.zeros'\cf0 , \cf4 TRUE\cf0 )\cf5 \
\pard\pardeftab720\sl300

\f1\fs24 \cf0 \cb1 \
\pard\pardeftab720\sl280
\cf0 names <- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")\
sr <- as.data.frame(matrix(0,nrow=20,ncol=6))\
mean <- as.data.frame(matrix(0,nrow=20,ncol=6))\
sd <- as.data.frame(matrix(0,nrow=20,ncol=6))\
for(i in 1:20)\{\
	filenames.output <- list.files(full.names = TRUE, path='../output/', pattern=paste("(^",i,")_.*\\\\output.txt$", sep=""))\
	if(length(filenames.output) != 0)\{\
		results <- lapply(filenames.output, read.table)\
		names(results) <- paste0('run',seq_along(results))\
		c_list <- ldply(results)\
		sr[i,] <-c(apply(c_list[,2:6], 2, function(x) length(which(x < gens[i]))/length(x)), length(unique(c_list$.id)))\
		mean[i, ] <- c(apply(c_list[,2:6], 2, function(x) mean(x)), length(unique(c_list$.id)))\
		sd[i,] <-c(apply(c_list[,2:6], 2, function(x) sd(x)), length(unique(c_list$.id)))\
	\}\
\}\
rownames(sr) = names\
rownames(mean) = names\
rownames(sd) = names\
sr <- rename(sr, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))\
mean <- rename(mean, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))\
sd <- rename(sd, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))\
set.caption('Success Ratio over given runs (Measure of share of runs which found all global optima)')\
pander(sr)\
\
```\
\

\f0\fs28 The second measure introduced by the CEC committee and also used by Dr. Fieldsend is the Convergence Rate. The Convergence Rate (CR) measures the needed evaluations per Accuracy and Function to find all global optima [@li_2013, p.7]. This measure takes the mean of evaluations over all runs. The results of this measure can be found in Table 3.\
\
$$\\frac\{\cf7 \\sum\\nolimits\cf8 _\{n\cf9 =1\cf8 \}^\{NR\} evals_\{n\}\}\{NR\} = CR$$ \
In this measure, $evals$ denotes the number of evaluations done. 
\f1\fs24 \cf0 \
\\newline\
```\{r, Mean, echo=FALSE\}\
set.caption('Convergence Rates over given runs (Mean of evaluations needed to find all global optima, if all optima have never been found the maximum allowed evaluations for that function were taken.)')\
pander(mean)\
```\
\

\f0\fs28 The third measure is the Peak Ratio (PR). It measures the share of found global optima over all runs [@li_2013, p.7]. The results of this evaluation can be found in Table 4.
\f3 \

\f1\fs24 \

\f0\fs28 $$\\frac\{\\\cf7 sum\\nolimits\cf8 _\{n\cf9 =1\cf8 \}^\{NR\} NOF_\{n\}\}\{NKO * NR\} = PR$$\
\\newline\
In this measure $NOF$ denotes the number of found optima per run and $NKO$ the number of known optima for the function. 
\f1\fs24 \cf0 \
\\newline\
```\{r, MPR, echo=FALSE\}\
source("../R/cec_2015_problem_data.R")\
names <- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")\
mpr <- as.data.frame(matrix(0,nrow=20,ncol=6))\
for(i in 1:20)\{\
	filenames.output <- list.files(full.names = TRUE, path='../output/', pattern=paste("(^",i,")_.*\\\\clist.txt$", sep=""))\
	if(length(filenames.output) != 0)\{\
		results <- lapply(filenames.output, read.table)\
		max <- lapply(results, function (x) sapply(x, max))\
		names(max) <- paste0('run',seq_along(max))\
		c_list <- ldply(max)\
		mpr[i,] <-c(apply(c_list[,2:6], 2, function(x) mean(x)/nopt[i]), length(unique(c_list$.id)))\
	\}\
\}\
rownames(mpr) = names\
mpr <- rename(mpr, c("V1" = "0.1", "V2" = "0.01", "V3" = "0.001", "V4" = "0.0001", "V5" = "0.00001", "V6" = "runs"))\
set.caption('Peak Ratio over given runs (Share of found global optima over all runs)')\
pander(mpr)\cf5 \cb3 \
\cf0 \cb1 \
```\
\pard\pardeftab720\sl280

\f0\fs28 \cf8 \
\pard\pardeftab720\sl280
\cf0 As a fourth measure, which wasn't introduced by the CEC committee, but used in the original nmmso implementation [@fieldsend_2014], the Number of Swarms was chosen. Since this is a continuous measure and, therefore no calculation is needed, this measure is pictured as graphs. The graphs can be found in Figure 1. They show the development of $number$ $of$ $swarms$ kept by nmmso.R over all iterations. Important to notice here is that $iterations$ is different from the $evaluations$ referenced in the other measures. Iterations are calls to start single runs of nmmso.R and, therefore, are different from the evaluations taken within the program.\
\
Additionally, a fifth measure was introduced which denotes the runtime of nmmso.R for the single functions. These times were taken on the ZIVHPC, a scientific High Perfomance Computing Cluster by Westf\'e4lische Wilhelms-Universit\'e4t M\'fcnster. Since the nmmso.R is a strictly sequential algorithm the runtimes for single runs will be comparable on common computers. The ZIVHPC was only used to parallelise the single runs.
\f1\fs24 \
\
\
![plot of chunk trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs. These plots contain all runs pictured in the other tables which were done for the benchmarking of this paper.](figure/trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs.-1.pdf) \
\
\
```\{r, trend curve of kept swarms over all 20 functions. The red curves show the number of swarms kept for each single run. The black line shows the mean of kept swarms over these runs., echo = FALSE, dpi = 250, fig.ext='pdf', keep='all'\}\
# rm(list = ls())\
# source("../R/cec_2015_problem_data.R")\
# library(plyr)\
# library(ggplot2)\
# library(ggthemes)\
# library(gridExtra)\
# library(grid)\
\
# plots = list()\
# for(i in 1:20)\{\
# 	results <- NA\
# 	filenames.swarms <- list.files(full.names = TRUE, path='../output', pattern=paste("(^",i,")(_.*)([[:digit:]].txt)$", sep=""))\
\
# 	results <- lapply(filenames.swarms, read.table)\
# 	names(results) <- paste0('run',seq_along(results))\
# 	results <- ldply(results)\
\
# 	results <- rename(results, c("V1" = "iterations", "V2" = "swarms"))\
\
# 	#results <- transform(results, run = sprintf('run%d', run)) \
\
# 	p1 = ggplot(results, aes(x = iterations, y = swarms, group=.id, colour="grey"))\
# 	p1 = p1 + ggtitle(paste0("F",i)) + geom_line() + theme_tufte() + theme(legend.position = "none")\
# 	p1 = p1 + stat_summary(fun.y = mean, geom="line", colour="black", aes(group = 1))\
# 	plots[[i]] = p1 + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(plot.margin=unit(c(0,0,0,0),"mm"))\
# \}\
# do.call("grid.arrange", c(plots, ncol = 4))\
```\
\

\f0\fs28 When comparing those measures with the ones given in the original paper [@fieldsend_2014], it can be seen that the reimplementation nmmso.R is an overall good resemblance of the original algorithm. The three CEC measures are close to the original taken measures and the trend curves for the number of kept swarms have similar trends. \
\
The biggest differences between the benchmarking results of the two implementations can be seen in the general results of function 14, 15, 16 and 18, as well as in the number of created swarms for the n-dimensional functions: \
\
(1) Function 14 and 15 have a $Success\\ Ratio$ of $1$ aswell as $Peak$ $Ratio$ of one $1$ for all accuracy levels. Additionally nmmso.R sometimes found all global optima for Function 18.  In contradiction of all three function almost never result in the finding of global optima in the evaluation of the original implementation. Only at the lowest accuracy, the original implementation is able to find all global optima for Function 14 [@fieldsend_2014, p.16].  It is hard to say if this difference is equal to an error in the implementation of nmmso.R or if an error in the original implementation was fixed. Also, this could be a difference in the reimplementation of the CEC Benchmarking Tool. Nevertheless, this is an interesting point of discussion and worth evaluating.\
\
(2) nmmso.R performs noticeable worse for Function 16 than for the original function. While nmmso.R has a $Peak$ $Ratio$ of $0.01$ for an accuracy of $0.1$ and $0$ for all others, the original implementation reaches a $Peak$ $Ratio$ of around $0.6$ for all accuracies. This might be an implementation error in the CEC Benchmarking Tool and not in nmmso.R since it is so significantly worse that it is unlikely that this difference would only occur in one test function.\
\
(3) Almost all algorithm runs on high-dimensional functions (F12-F20) result in a high number of swarms while all other results regarding this functions are comparable to the original results. This difference becomes very clear in the case of Functions 17-20. In the paper addressing the original implementation the x-axis rank from 0-40,000 iterations while for the reimplementation limit of 4,000 for Function 17, of 20,000 for Function 18, 6,000 for Function 19 and of 30,000 for Function 20 is enough to show all data sets. This is connected to the creation of much more swarms, which leads to an earlier depletion of the maximum allowed number of evaluations.\
\
Additionally, the time of all algorithms was taken. Even though this measure widely varies depending on the computer's architecture it can show the different complexity of all 20 functions.\

\f1\fs24 \
```\{r, time consumption, echo=FALSE\}\
names <- c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12", "F13", "F14", "F15", "F16", "F17", "F18", "F19", "F20")\
times <- as.data.frame(matrix(0, nrow = 20, ncol = 2))\
for(i in 1:20)\{\
	filenames.times <- list.files(full.names = TRUE, path='../output', pattern=paste("(^",i,")_.*\\\\_time.txt$", sep=""))\
	if(length(filenames.times) != 0)\{\
		results <- lapply(filenames.times, read.table)\
		time <- ldply(results)\
		mean <- mean(time$V3)\
		sd <- sd(time$V3)\
		times[i,] <- c(mean, sd)\
	\}\
\}\
rownames(times) = names\
times <- rename(times, c("V1" = "Mean", "V2" = "Standard Deviation"))\
\
set.caption('Taken time of nmmso.R for all 20 functions. All times are in seconds.')\
pander(times)\
```}